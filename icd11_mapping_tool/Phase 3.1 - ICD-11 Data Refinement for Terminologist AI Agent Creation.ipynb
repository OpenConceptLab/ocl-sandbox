{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94cbdda-1926-4210-a13d-912928941d8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction - Support Services for LLM Training and AI Agent Creation for Clinical Concept Coding in ICD-11\n",
    "\n",
    "The process of coding clinical concepts requires specific technical knowledge of each terminology and its structure. ICD-11 introduces relevant structural principles to maintain a solid foundation where codes are arranged either in groups or as standalone entities, with lexical expansion capabilities. Some key elements include:\n",
    "\n",
    "1. **Foundation codes** refer to each individual ICD-11 concept, represented by an Entity ID. These are organized into chapters, which are further divided into blocks.\n",
    "2. **Stem codes** are the codes that directly represent a clinical entity, such as a disease, examination finding, or symptom.\n",
    "3. **Extension codes** are codes from Chapter X (all starting with the letter X). They cannot be used alone to represent a disease and must always accompany a stem code.\n",
    "4. **Leaf codes** are terminal codes, meaning there is no more specific individual concept below them for that subject—no further subcategory exists.\n",
    "\n",
    "Here, we will develop strategies to automate the mapping of pre-coordinated CIEL clinical concepts to ICD-11 codes. Some of these will require post-coordination, a mechanism in ICD-11 for combining codes into clusters to add specificity in areas such as laterality, anatomical site, pathological history, histological aspect, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8baca-4b5d-47a9-a353-075bb4e057e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installing requirements on Linux Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909652f6-c7e8-41ae-8a0d-5933f5f73bf7",
   "metadata": {},
   "source": [
    "## Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453ef69-63c0-482c-93c9-86c7e61d1743",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 1. Update package index\n",
    "sudo apt update\n",
    "\n",
    "# 2. Install dependencies for repository management over HTTPS\n",
    "sudo apt install -y ca-certificates curl gnupg lsb-release\n",
    "\n",
    "# 3. Add Docker's official GPG key\n",
    "sudo install -m 0755 -d /etc/apt/keyrings\n",
    "curl -fsSL https://download.docker.com/linux/ubuntu/gpg \\\n",
    "  | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n",
    "\n",
    "# 4. Set up the Docker repository\n",
    "echo \\\n",
    "  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \\\n",
    "  https://download.docker.com/linux/ubuntu \\\n",
    "  $(. /etc/os-release && echo \\\"$VERSION_CODENAME\\\") stable\" \\\n",
    "  | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "\n",
    "# 5. Update package index again and install Docker Engine + Compose plugin\n",
    "sudo apt update\n",
    "sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n",
    "\n",
    "# 6. Verify Docker Engine and Compose installation\n",
    "docker --version          # Check Docker Engine version\n",
    "docker compose version    # Check Docker Compose plugin version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a74c1a-b9b1-4f39-a5a7-6a48eb0c7fa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assets\n",
    "\n",
    "In order to create a tool that facilitates the work of terminologists, the first step is to have access to ICD-11 data. For this, we have at our disposal the [ICD API](https://icd.who.int/docs/icd-api/ICDAPI-DockerContainer/) provided by WHO itself. However, it comes with the inconvenience of slow queries, fragmented data, and difficulty accessing the database directly.\n",
    "\n",
    "All the data used in this application will [most likely be available in this Google Drive folder](https://drive.google.com/drive/folders/15rapHa1uWNQQtqmI0ciVpNuh93c4LBs4?hl=pt-br)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d1c35-0a97-4b48-84ea-0a9d18ad94df",
   "metadata": {},
   "source": [
    "## Running ICD API\n",
    "\n",
    "```bash\n",
    "docker run --name icdapi -p 8080:80 -e acceptLicense=true -e saveAnalytics=true whoicd/icd-api\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6380e-ccb0-4dc5-84ad-99e69f315d90",
   "metadata": {},
   "source": [
    "## Vectorizing ICD Data\n",
    "\n",
    "Para aumentar a velocidade de busca e facilitar o uso desses dados com métodos como RAG os dados foram vetorizados utilizando diferentes abordagens para comparação de eficiência e até mesmo uso combinado das abordagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb897de8-904d-4b9b-8878-69ad04427dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "    qdrant-client[fastembed] \\ \n",
    "    transformers==4.52.3 \\\n",
    "    sentence-transformers==4.1.0 \\\n",
    "    pandas \\\n",
    "    torch==2.7.0 \\\n",
    "    torchaudio==2.7.0 \\\n",
    "    torchvision==0.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea8247-a96d-465a-aa3d-0f8efc36fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# detect device (GPU if available)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# 1. download and extract the WHO ICD-11 simple tabulation zip\n",
    "icd_url = \"https://icdcdn.who.int/static/releasefiles/2025-01/SimpleTabulation-ICD-11-MMS-en.zip\"\n",
    "response = requests.get(icd_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as archive:\n",
    "    archive.extract(\"SimpleTabulation-ICD-11-MMS-en.xlsx\", path=\"./\")\n",
    "    with archive.open(\"SimpleTabulation-ICD-11-MMS-en.xlsx\") as xlsx_file:\n",
    "        df = pd.read_excel(xlsx_file)\n",
    "\n",
    "# 2. filter rows where isLeaf is True and sample 5000 titles\n",
    "leaf_df = df[df['isLeaf'] == 'True']\n",
    "sample_df = leaf_df.sample(n=5000, random_state=42)\n",
    "\n",
    "# 3. extract Title column and strip any leading hyphens/spaces\n",
    "texts = (\n",
    "    sample_df['Title']\n",
    "    .str.replace(r'^[-\\s]+', '', regex=True)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "def benchmark(model_name, corpus):\n",
    "    print(f\"\\nBenchmarking: {model_name}\")\n",
    "    model = SentenceTransformer(model_name).to(device)\n",
    "    start_time = time.time()\n",
    "    batch_size = 32\n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=model_name):\n",
    "        batch = corpus[i : i + batch_size]\n",
    "        _ = model.encode(batch, show_progress_bar=False, convert_to_numpy=True)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"→ {model_name}: {len(corpus)} embeddings in {elapsed:.2f} seconds\")\n",
    "    return elapsed\n",
    "\n",
    "# list of models including SapBERT\n",
    "model_list = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"pritamdeka/S-BioBert-snli-multinli-stsb\",\n",
    "    \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\"\n",
    "]\n",
    "\n",
    "# run benchmarks\n",
    "timings = {}\n",
    "for name in model_list:\n",
    "    timings[name] = benchmark(name, texts)\n",
    "\n",
    "# compute speedups relative to MiniLM\n",
    "mini_time = timings[model_list[0]]\n",
    "mpnet_time = timings[model_list[1]]\n",
    "bio_time = timings[model_list[2]]\n",
    "sapbert_time = timings[model_list[3]]\n",
    "\n",
    "speedup_mpnet = mpnet_time / mini_time\n",
    "print(f\"\\n🧪 MiniLM is {speedup_mpnet:.2f}× faster than MPNet\")\n",
    "\n",
    "speedup_bio = bio_time / mini_time\n",
    "print(f\"🧪 MiniLM is {speedup_bio:.2f}× faster than S-BioBERT\")\n",
    "\n",
    "speedup_sap = sapbert_time / mini_time\n",
    "print(f\"🧪 MiniLM is {speedup_sap:.2f}× faster than SapBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53319fc9-105a-4fc4-a920-21913faa21dc",
   "metadata": {},
   "source": [
    "## Extracting data from the ICD API\n",
    "\n",
    "The [table provided by ICD-11](https://icd.who.int/docs/icd-api/ICDAPI-DockerContainer/) contains many data fields, but it does not include linearization details, synonyms, or relationships between elements (index terms).  \n",
    "\n",
    "For this reason, we created a process to extract data from the ICD API and generate relational data tables, and finally produce a JSON file with the summarized data.\n",
    "\n",
    "We will create a database to work with these intermediate tables.\n",
    "\n",
    "```bash\n",
    "docker run --name db \\\n",
    "  -e MYSQL_ROOT_PASSWORD=mypass \\\n",
    "  -e MYSQL_DATABASE=icd11 \\\n",
    "  -p 3306:3306 \\\n",
    "  -v mariadbdata:/var/lib/mysql \\\n",
    "  -d mariadb:11.8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661d254-88f1-4d86-8346-f52b06f86cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\n",
    "    \"mysql+pymysql://root:mypass@localhost:3306/icd11\",\n",
    "    echo=False,\n",
    "    pool_pre_ping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac35858-ce6d-47ae-b76d-a589f9a0aceb",
   "metadata": {},
   "source": [
    "### Generating some auxiliary CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b19c8-850c-4a13-b5c2-aecd61e4a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Constantes da API\n",
    "BASE_ENTITY_URL = \"http://localhost:8080/icd/entity/\"\n",
    "BASE_LINEAR_URL = \"http://localhost:8080icd/release/11/2025-01/mms/\"\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"API-Version\": \"v2\",\n",
    "    \"Accept-Language\": \"en\"\n",
    "}\n",
    "\n",
    "# Auxiliares\n",
    "visited = set()\n",
    "rows = []\n",
    "\n",
    "def get_entity(entity_id):\n",
    "    url = BASE_ENTITY_URL + entity_id\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def get_code_for_entity_path(entity_path):\n",
    "    \"\"\"\n",
    "    Recebe o que vem após /mms/, como '1673130746' ou '927970860/unspecified'\n",
    "    \"\"\"\n",
    "    url = BASE_LINEAR_URL + entity_path\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get(\"code\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_entity_path(full_url):\n",
    "    \"\"\"Extrai o caminho completo após /mms/\"\"\"\n",
    "    try:\n",
    "        return full_url.split(\"/mms/\")[1]\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "\n",
    "def extract_entity_id(full_url):\n",
    "    \"\"\"Extrai apenas o último ID numérico da URL (para a base foundation)\"\"\"\n",
    "    return full_url.rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "def traverse(entity_id, pbar):\n",
    "    if entity_id in visited:\n",
    "        return\n",
    "    visited.add(entity_id)\n",
    "\n",
    "    try:\n",
    "        data = get_entity(entity_id)\n",
    "    except Exception:\n",
    "        pbar.update(1)\n",
    "        return\n",
    "\n",
    "    entity_url = data.get(\"@id\", \"\")\n",
    "    entity_id_str = extract_entity_id(entity_url)\n",
    "    title = data.get(\"title\", {}).get(\"@value\", \"\")\n",
    "    code = get_code_for_entity_path(entity_id_str)\n",
    "\n",
    "    parents = data.get(\"parent\", [])\n",
    "    if not parents:\n",
    "        rows.append([entity_id_str, code, title, \"\", \"\"])\n",
    "    else:\n",
    "        for parent_url in parents:\n",
    "            parent_id = extract_entity_id(parent_url)\n",
    "            parent_code = get_code_for_entity_path(parent_id)\n",
    "            rows.append([entity_id_str, code, title, parent_id, parent_code])\n",
    "\n",
    "    for child_url in data.get(\"child\", []):\n",
    "        child_id = extract_entity_id(child_url)\n",
    "        traverse(child_id, pbar)\n",
    "\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840809bd-81bd-465e-bb6e-2de6b662930c",
   "metadata": {},
   "source": [
    "#### icd_flat_hierarchy\n",
    "\n",
    "Stores the hierarchy of all entities (foundation codes) from ICD-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b17b7-af7d-46ea-a6ff-f506f289a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "start_entity_id = \"448895267\"  # Root entity for ICD Entities\n",
    "\n",
    "print(\"📦 Starting ICD hierarchy collection…\")\n",
    "with tqdm(total=1, desc=\"Building hierarchy\", unit=\"nodes\") as pbar:  # Expected ~70,702 nodes\n",
    "    traverse(start_entity_id, pbar)\n",
    "\n",
    "# Build the DataFrame and save as CSV\n",
    "df = pd.DataFrame(\n",
    "    rows, \n",
    "    columns=[\"entity_id\", \"code\", \"title\", \"parent_entity_id\", \"parent_code\"]\n",
    ")\n",
    "df.to_csv(\"./icd_flat_hierarchy.csv\", index=False)  # Final size: ~78,733 rows\n",
    "\n",
    "print(\"✅ CSV file 'icd_flat_hierarchy.csv' generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079aa22-ca35-42bc-b3d5-3c6c4a35dd30",
   "metadata": {},
   "source": [
    "#### icd_linear_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee519370-57b6-4417-99ed-b1e4121f9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "\n",
    "# já há um engine SQLAlchemy definido em outro bloco\n",
    "query = text('SELECT * FROM icd11_mms_en')\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "BASE_LINEAR_URL = \"http://localhost:8080/icd/release/11/2025-01/mms/\"\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"API-Version\": \"v2\",\n",
    "    \"Accept-Language\": \"en\"\n",
    "}\n",
    "\n",
    "final_rows = []\n",
    "processed_count = 0\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processando icd11_mms_en\"):\n",
    "    lin_uri = row['linearization_uri']\n",
    "    if pd.isna(lin_uri) or not lin_uri:\n",
    "        continue\n",
    "\n",
    "    # extrai tudo após /mms/, preservando /unspecified, /other etc.\n",
    "    eid_path = lin_uri.split(\"/mms/\")[1]\n",
    "    foundation_entity_id = eid_path.split(\"/\")[0]\n",
    "\n",
    "    # busca linearização\n",
    "    resp = requests.get(BASE_LINEAR_URL + eid_path, headers=HEADERS)\n",
    "    if resp.status_code != 200:\n",
    "        continue\n",
    "    data = resp.json()\n",
    "\n",
    "    main_code = data.get(\"code\", \"\")\n",
    "    main_title = data.get(\"title\", {}).get(\"@value\", \"\")\n",
    "    # limpa hífens à esquerda\n",
    "    main_title = re.sub(r'^-+\\s*', '', main_title).strip()\n",
    "\n",
    "    postcoord = data.get(\"postcoordinationScale\", [])\n",
    "    if not postcoord:\n",
    "        final_rows.append([\n",
    "            foundation_entity_id,\n",
    "            main_code,\n",
    "            main_title,\n",
    "            \"\", \"\", \"\"\n",
    "        ])\n",
    "        processed_count += 1\n",
    "    else:\n",
    "        added = False\n",
    "        for item in postcoord:\n",
    "            for se_url in item.get(\"scaleEntity\", []):\n",
    "                se_path = se_url.split(\"/mms/\")[1]\n",
    "                ext_entity_id = se_path.split(\"/\")[0]\n",
    "                r2 = requests.get(BASE_LINEAR_URL + se_path, headers=HEADERS)\n",
    "                if r2.status_code != 200:\n",
    "                    continue\n",
    "                d2 = r2.json()\n",
    "                ext_code  = d2.get(\"code\", \"\")\n",
    "                ext_title = d2.get(\"title\", {}).get(\"@value\", \"\")\n",
    "                ext_title = re.sub(r'^-+\\s*', '', ext_title).strip()\n",
    "\n",
    "                final_rows.append([\n",
    "                    foundation_entity_id,\n",
    "                    main_code,\n",
    "                    main_title,\n",
    "                    ext_entity_id,\n",
    "                    ext_code,\n",
    "                    ext_title\n",
    "                ])\n",
    "                added = True\n",
    "        if added:\n",
    "            processed_count += 1\n",
    "\n",
    "print(f\"Registros processados: {processed_count}\")\n",
    "\n",
    "# monta DataFrame e salva\n",
    "cols = [\n",
    "    \"entity_id\",\n",
    "    \"code\",\n",
    "    \"title\",\n",
    "    \"extension_entity_id\",\n",
    "    \"extension_code\",\n",
    "    \"extension_title\"\n",
    "]\n",
    "out = pd.DataFrame(final_rows, columns=cols)\n",
    "out.to_csv(\"./assets/icd_linear_extensions.csv\", index=False)\n",
    "print(\"✅ icd_linear_extensions.csv gerado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6f0cb-d7f6-42bb-9d42-c994d2ccc42b",
   "metadata": {},
   "source": [
    "### Generating auxiliary tables\n",
    "\n",
    "Capturing the relevant data from the ICD API database and converting it into a relational data structure that we are familiar with was the first step towards creating a dataset based on this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89567d-3be4-4046-aab7-d5473a2a1c9b",
   "metadata": {},
   "source": [
    "#### icd11_mms_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc462da3-de85-4d75-be7d-5e62ece3d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import urllib3\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Disable SSL warnings from requests\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = \"./SimpleTabulation-ICD-11-MMS-en.xlsx\"\n",
    "\n",
    "# Function to normalize column names\n",
    "def normalize_column_name(name):\n",
    "    name = name.strip().replace(\" \", \"_\")\n",
    "    s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    s2 = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "    return s2.lower()\n",
    "\n",
    "# Read the Excel file and normalize column names\n",
    "df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "df.columns = [normalize_column_name(col) for col in df.columns]\n",
    "print(\"Renamed columns:\", df.columns.tolist())\n",
    "\n",
    "# Select only expected columns\n",
    "expected_columns = [\n",
    "    'foundation_uri', 'linearization_uri', 'code', 'block_id', 'title', \n",
    "    'class_kind', 'depth_in_kind', 'is_residual', 'chapter_no', \n",
    "    'browser_link', 'is_leaf', 'primary_tabulation', 'grouping1', 'grouping2', 'grouping3', 'grouping4', 'grouping5'\n",
    "]\n",
    "df = df[expected_columns]\n",
    "\n",
    "# Drop existing table if it exists\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS icd11_mms_en;\"))\n",
    "print(\"Old icd11_mms_en table dropped (if existed).\")\n",
    "\n",
    "# Create new table\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE icd11_mms_en (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            foundation_uri TEXT,\n",
    "            linearization_uri TEXT,\n",
    "            code TEXT,\n",
    "            block_id TEXT,\n",
    "            title TEXT,\n",
    "            class_kind TEXT,\n",
    "            depth_in_kind TEXT,\n",
    "            is_residual BOOLEAN,\n",
    "            chapter_no TEXT,\n",
    "            browser_link TEXT,\n",
    "            is_leaf BOOLEAN,\n",
    "            primary_tabulation TEXT,\n",
    "            grouping1 TEXT,\n",
    "            grouping2 TEXT,\n",
    "            grouping3 TEXT,\n",
    "            grouping4 TEXT,\n",
    "            grouping5 TEXT\n",
    "        );\n",
    "    \"\"\"))\n",
    "print(\"New icd11_mms_en table created.\")\n",
    "\n",
    "# Handle missing values\n",
    "df = df.where(pd.notnull(df), None)\n",
    "records = df.to_dict(orient=\"records\")\n",
    "for record in records:\n",
    "    for key, value in record.items():\n",
    "        if isinstance(value, float) and math.isnan(value):\n",
    "            record[key] = None\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = text(\"\"\"\n",
    "INSERT INTO icd11_mms_en\n",
    "(foundation_uri, linearization_uri, code, block_id, title, class_kind, depth_in_kind, is_residual, chapter_no, browser_link, is_leaf, primary_tabulation, grouping1, grouping2, grouping3, grouping4, grouping5)\n",
    "VALUES (:foundation_uri, :linearization_uri, :code, :block_id, :title, :class_kind, :depth_in_kind, :is_residual, :chapter_no, :browser_link, :is_leaf, :primary_tabulation, :grouping1, :grouping2, :grouping3, :grouping4, :grouping5)\n",
    "\"\"\")\n",
    "if records:\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(insert_query, records)\n",
    "    print(f\"Data inserted successfully! Total records: {len(records)}\")\n",
    "else:\n",
    "    print(\"No records to insert.\")\n",
    "\n",
    "# Create indexes to improve query performance (fixed for TEXT columns)\n",
    "create_indexes = [\n",
    "    \"CREATE INDEX idx_icd11_is_leaf ON icd11_mms_en (is_leaf);\",\n",
    "    \"CREATE INDEX idx_icd11_code ON icd11_mms_en (code(50));\",\n",
    "    \"CREATE INDEX idx_icd11_linearization_uri ON icd11_mms_en (linearization_uri(100));\"\n",
    "]\n",
    "\n",
    "with engine.begin() as connection:\n",
    "    for index_sql in create_indexes:\n",
    "        connection.execute(text(index_sql))\n",
    "print(\"Indexes created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f5014f-0d24-4ecc-a88f-91fd5f905a54",
   "metadata": {},
   "source": [
    "#### icd11_mms_en_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3e9b9-efe9-4cb0-a313-3955f8afbba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Create the target table\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS analytics.icd11_mms_en_name;\"))\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE analytics.icd11_mms_en_name (\n",
    "            icd11_mms_en_id INT NOT NULL,\n",
    "            name VARCHAR(200) NOT NULL,\n",
    "            name_type VARCHAR(10) NOT NULL,\n",
    "            PRIMARY KEY (icd11_mms_en_id, name, name_type),\n",
    "            FOREIGN KEY (icd11_mms_en_id) REFERENCES analytics.icd11_mms_en(id)\n",
    "        );\n",
    "    \"\"\"))\n",
    "\n",
    "# 2. Collect names and synonyms\n",
    "results = []\n",
    "for _, row in tqdm(icd11_df.iterrows(), total=len(icd11_df), desc=\"Processing names\"):\n",
    "    icd_id = row['id']\n",
    "    entity_id_residual = row['entity_id_residual']\n",
    "    if not pd.notnull(entity_id_residual):\n",
    "        continue\n",
    "    url = f\"{BASE_LINEAR_URL}{entity_id_residual}\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    if r.status_code != 200:\n",
    "        continue\n",
    "    data = r.json()\n",
    "    \n",
    "    # Extract FSN (Fully Specified Name)\n",
    "    fsn = data.get(\"title\", {}).get(\"@value\")\n",
    "    if fsn:\n",
    "        results.append((icd_id, fsn, \"fsn\"))\n",
    "    \n",
    "    # Extract synonyms and remove duplicates (FSN cannot appear as synonym)\n",
    "    synonyms = set()\n",
    "    for idx_term in data.get(\"indexTerm\", []):\n",
    "        label = idx_term.get(\"label\", {}).get(\"@value\")\n",
    "        if label and label != fsn:\n",
    "            synonyms.add(label)\n",
    "    for syn in synonyms:\n",
    "        results.append((icd_id, syn, \"synonym\"))\n",
    "\n",
    "# 3. Save results as a DataFrame\n",
    "names_df = pd.DataFrame(results, columns=[\"icd11_mms_en_id\", \"name\", \"name_type\"])\n",
    "\n",
    "# 4. Insert data in batches\n",
    "batch_size = 200\n",
    "with engine.begin() as conn:\n",
    "    for i in tqdm(range(0, len(names_df), batch_size), desc=\"Inserting names\"):\n",
    "        batch = names_df.iloc[i:i+batch_size]\n",
    "        values = [tuple(row) for row in batch.values]\n",
    "        conn.execute(\n",
    "            text(\"\"\"\n",
    "                INSERT IGNORE INTO analytics.icd11_mms_en_name\n",
    "                (icd11_mms_en_id, name, name_type)\n",
    "                VALUES (:icd11_mms_en_id, :name, :name_type)\n",
    "            \"\"\"),\n",
    "            [dict(icd11_mms_en_id=icd_id, name=name, name_type=ntype) for icd_id, name, ntype in values]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d76345-6202-42da-ada9-28b8fa34da7b",
   "metadata": {},
   "source": [
    "#### icd11_mms_en_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b31ed9-00d8-4b0c-bf97-90d5589dc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import text\n",
    "\n",
    "# 1. Drop and recreate the table\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS icd11_mms_en_hierarchy;\"))\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE icd11_mms_en_hierarchy (\n",
    "        id INT(11) NOT NULL AUTO_INCREMENT,\n",
    "        entity_id VARCHAR(50) NOT NULL,\n",
    "        code VARCHAR(50) NULL,\n",
    "        title TEXT NULL,\n",
    "        parent_entity_id VARCHAR(50) NULL,\n",
    "        parent_code VARCHAR(50) NULL,\n",
    "        PRIMARY KEY (id)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n",
    "    \"\"\"))\n",
    "\n",
    "# 2. Configuration\n",
    "csv_path = \"./icd_flat_hierarchy.csv\"\n",
    "batch_size = 500\n",
    "total_rows = sum(1 for _ in open(csv_path, encoding=\"utf-8\")) - 1\n",
    "total_batches = total_rows // batch_size + 1\n",
    "\n",
    "# 3. Insert statement\n",
    "insert_sql = text(\"\"\"\n",
    "    INSERT INTO icd11_mms_en_hierarchy (\n",
    "        entity_id, code, title, parent_entity_id, parent_code\n",
    "    ) VALUES (\n",
    "        :entity_id, :code, :title, :parent_entity_id, :parent_code\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# 4. Read and insert data in batches, replacing NaN with None\n",
    "reader = pd.read_csv(csv_path, chunksize=batch_size, encoding=\"utf-8\")\n",
    "\n",
    "for chunk in tqdm(reader, total=total_batches, desc=\"Inserting batches\"):\n",
    "    chunk = chunk.where(pd.notnull(chunk), None)  # replace NaN with None\n",
    "    records = chunk.to_dict(orient=\"records\")\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(insert_sql, records)\n",
    "\n",
    "print(\"✅ Table icd11_mms_en_hierarchy recreated and data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13bdfb-e2a7-42a7-84b5-3a3605ebc4dc",
   "metadata": {},
   "source": [
    "#### icd11_mms_en_postcoordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488cb82-f57b-4e95-9664-afb84e107941",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install httpx tqdm pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747af58-99b6-43cc-8956-74773197bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from functools import lru_cache\n",
    "from tqdm.asyncio import tqdm\n",
    "from tqdm import tqdm as tqdm_sync\n",
    "from sqlalchemy.engine import Engine\n",
    "import gc\n",
    "import nest_asyncio\n",
    "\n",
    "# --- Prerequisites: icd11_df, BASE_LINEAR_URL, HEADERS, and engine must be defined ---\n",
    "\n",
    "# --- 0. Define the set of terminal leaf entity_id_residual ---\n",
    "leaf_entity_ids = set(\n",
    "    icd11_df.loc[icd11_df['is_leaf'] == 1, 'entity_id_residual'].dropna()\n",
    ")\n",
    "\n",
    "# --- 1. Rebuild the child map from the flattened hierarchy ---\n",
    "flat = pd.read_csv(\"../assets/icd_flat_hierarchy.csv\", dtype=str).fillna(\"\")\n",
    "children_map = flat.groupby('parent_entity_id')['entity_id'].apply(list).to_dict()\n",
    "\n",
    "# --- 2. Preload code and title information into a dictionary for O(1) access ---\n",
    "leaf_info = (\n",
    "    icd11_df.set_index(\"entity_id_residual\")[[\"code\", \"title\"]]\n",
    "    .dropna(subset=[\"code\"])\n",
    "    .to_dict(\"index\")\n",
    ")\n",
    "\n",
    "# --- 3. Memoize the leaf descendant search ---\n",
    "@lru_cache(maxsize=None)\n",
    "def get_leaf_descendants_cached(start_eid: str) -> set[str]:\n",
    "    stack, leafs = [start_eid], set()\n",
    "    while stack:\n",
    "        cur = stack.pop()\n",
    "        for child in children_map.get(cur, []):\n",
    "            if child in leaf_entity_ids:\n",
    "                leafs.add(child)\n",
    "            else:\n",
    "                stack.append(child)\n",
    "    return leafs\n",
    "\n",
    "# --- 4. Helper function to extract entity ID from linearization URI ---\n",
    "def get_entity_id_from_linearization_uri(uri: str, residual: bool = True) -> str:\n",
    "    return uri.split(\"/\")[-1] if residual else uri\n",
    "\n",
    "# --- 5. Main API request function ---\n",
    "CONCURRENCY = 30\n",
    "SEM = asyncio.Semaphore(CONCURRENCY)\n",
    "\n",
    "async def fetch_postcoord(icd_id: int, eid: str, client: httpx.AsyncClient):\n",
    "    url = f\"{BASE_LINEAR_URL}{eid}\"\n",
    "    async with SEM:\n",
    "        r = await client.get(url)\n",
    "    if r.status_code != 200:\n",
    "        return []\n",
    "    data = r.json()\n",
    "    rows = []\n",
    "\n",
    "    for scale in data.get(\"postcoordinationScale\", []):\n",
    "        required = scale.get(\"requiredPostcoordination\", \"false\").lower() == \"true\"\n",
    "        for scale_entity_url in scale.get(\"scaleEntity\", []):\n",
    "            child_residual = get_entity_id_from_linearization_uri(scale_entity_url, residual=True)\n",
    "            to_process = (\n",
    "                [child_residual] if child_residual in leaf_entity_ids\n",
    "                else get_leaf_descendants_cached(child_residual)\n",
    "            )\n",
    "            for leaf_eid in to_process:\n",
    "                leaf = leaf_info.get(leaf_eid)\n",
    "                if not leaf:\n",
    "                    continue\n",
    "                code_type = \"extension\" if leaf[\"code\"].startswith(\"X\") else \"stem\"\n",
    "                rows.append((icd_id, code_type, leaf[\"code\"], leaf[\"title\"], required))\n",
    "    return rows\n",
    "\n",
    "# --- 6. Async main loop ---\n",
    "async def build_rows():\n",
    "    tasks, results = [], []\n",
    "    async with httpx.AsyncClient(timeout=10, headers=HEADERS) as client:\n",
    "        for _, row in icd11_df.query(\"is_leaf == 1\").iterrows():\n",
    "            eid = row[\"entity_id_residual\"]\n",
    "            if eid:\n",
    "                tasks.append(fetch_postcoord(row[\"id\"], eid, client))\n",
    "\n",
    "        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"API\"):\n",
    "            results.extend(await coro)\n",
    "    return results\n",
    "\n",
    "# Enable nested event loop (for Jupyter or environments that already run an event loop)\n",
    "nest_asyncio.apply()\n",
    "rows = await build_rows()\n",
    "\n",
    "# --- 7. Create the target table in the database ---\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS analytics.icd11_mms_en_postcoordination;\"))\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE analytics.icd11_mms_en_postcoordination (\n",
    "            id INT NOT NULL AUTO_INCREMENT,\n",
    "            icd11_mms_en_id INT NOT NULL,\n",
    "            code_type VARCHAR(20) NOT NULL,\n",
    "            code VARCHAR(16) NOT NULL,\n",
    "            title TEXT NOT NULL,\n",
    "            required TINYINT(1) NOT NULL,\n",
    "            PRIMARY KEY (id)\n",
    "        );\n",
    "    \"\"\"))\n",
    "\n",
    "# --- 8. Insert data into the database in batches using tqdm ---\n",
    "postcoord_df = pd.DataFrame(rows, columns=[\"icd11_mms_en_id\", \"code_type\", \"code\", \"title\", \"required\"])\n",
    "postcoord_df.drop_duplicates(subset=[\"icd11_mms_en_id\", \"code\"], inplace=True)\n",
    "\n",
    "batch_size = 5000\n",
    "with engine.begin() as conn:\n",
    "    for i in tqdm_sync(range(0, len(postcoord_df), batch_size), desc=\"Inserting into database\"):\n",
    "        batch = postcoord_df.iloc[i:i+batch_size]\n",
    "        conn.execute(\n",
    "            text(\"\"\"\n",
    "                INSERT INTO analytics.icd11_mms_en_postcoordination\n",
    "                    (icd11_mms_en_id, code_type, code, title, required)\n",
    "                VALUES (:icd11_mms_en_id, :code_type, :code, :title, :required)\n",
    "            \"\"\"),\n",
    "            batch.to_dict(orient=\"records\")\n",
    "        )\n",
    "\n",
    "# --- 9. Memory cleanup ---\n",
    "del rows, postcoord_df, leaf_info\n",
    "get_leaf_descendants_cached.cache_clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf0630-6c05-406c-af8b-d0d692cb7f59",
   "metadata": {},
   "source": [
    "### Generating the final JSON\n",
    "\n",
    "By combining data from all tables, we created a JSON file that will serve as the source for populating the vector databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aabed4-f7e1-45cc-a0e0-7b5a22ef78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 1. Load DataFrames\n",
    "# icd11_df should already be loaded with columns: id, title, code, is_leaf, entity_id_residual\n",
    "\n",
    "print(\"Loading icd11_mms_en_name...\")\n",
    "names_df = pd.read_sql(\n",
    "    \"SELECT icd11_mms_en_id, name, name_type FROM analytics.icd11_mms_en_name\",\n",
    "    engine\n",
    ")\n",
    "\n",
    "print(\"Loading icd11_mms_en_postcoordination...\")\n",
    "postcoord_df = pd.read_sql(\"\"\"\n",
    "    SELECT icd11_mms_en_id, code, code_type, title\n",
    "    FROM analytics.icd11_mms_en_postcoordination\n",
    "\"\"\", engine)\n",
    "\n",
    "# 2. Join is_leaf and entity_id_residual for each code\n",
    "print(\"Creating postcoord_with_more dataset...\")\n",
    "postcoord_with_more = (\n",
    "    postcoord_df\n",
    "    .merge(\n",
    "        icd11_df[['code', 'is_leaf', 'entity_id_residual']],\n",
    "        on='code',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Group postcoordination options by stem_id for quick lookup\n",
    "postcoord_grouped = postcoord_with_more.groupby('icd11_mms_en_id')\n",
    "\n",
    "# 4. Build the list of JSON objects\n",
    "vector_input = []\n",
    "for _, row in tqdm(icd11_df.iterrows(), total=len(icd11_df), desc=\"Generating JSON\"):\n",
    "    cid = row['id']\n",
    "    title = row['title']\n",
    "    code = row['code']\n",
    "    is_leaf = bool(row['is_leaf'])\n",
    "    entity_id_residual = row['entity_id_residual']\n",
    "    \n",
    "    if not code:\n",
    "        code_type = \"foundation\"\n",
    "    elif str(code).startswith(\"X\"):\n",
    "        code_type = \"extension\"\n",
    "    else:\n",
    "        code_type = \"stem\"\n",
    "\n",
    "    metadata = {\n",
    "        \"code\": code,\n",
    "        \"entity_id_residual\": entity_id_residual,\n",
    "        \"code_type\": code_type,\n",
    "        \"name_type\": \"fsn\",\n",
    "        \"is_leaf\": is_leaf\n",
    "    }\n",
    "\n",
    "    opts = postcoord_grouped.get_group(cid) if cid in postcoord_grouped.groups else pd.DataFrame()\n",
    "\n",
    "    metadata[\"leaf_postcoordination_options\"] = [\n",
    "        {\n",
    "            \"code\": opt_row['code'],\n",
    "            \"code_type\": opt_row['code_type'],\n",
    "            \"title\": opt_row['title'],\n",
    "            \"is_leaf\": bool(opt_row['is_leaf']),\n",
    "            \"entity_id_residual\": opt_row['entity_id_residual']\n",
    "        }\n",
    "        for _, opt_row in opts.iterrows() if pd.notnull(opt_row['code'])\n",
    "    ]\n",
    "\n",
    "    vector_input.append({\n",
    "        \"concept_name\": title,\n",
    "        \"metadata\": metadata\n",
    "    })\n",
    "\n",
    "# 5. Save as JSON\n",
    "with open(\"icd11_vector_input.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vector_input, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Generated icd11_vector_input.json with {len(vector_input)} concepts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c1006-883c-4e04-ab3d-8d55182ad44c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce420069-398b-4a5a-81ea-02800118029d",
   "metadata": {},
   "source": [
    "## Run Qdrant vector database\n",
    "\n",
    "Accordingly to [docs](https://qdrant.tech/documentation/quickstart/)\n",
    "\n",
    "```bash\n",
    "docker pull qdrant/qdrant\n",
    "docker run -p 6333:6333 -p 6334:6334 \\\n",
    "    -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
    "    qdrant/qdrant\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415585c0-71a5-4ce6-bbe5-49de5bc86869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# --- MODELS AND COLLECTIONS ---\n",
    "model_infos = [\n",
    "    (\"sentence-transformers/all-MiniLM-L6-v2\",     \"icd11_concepts_minilm\"),\n",
    "    (\"sentence-transformers/all-mpnet-base-v2\",    \"icd11_concepts_mpnet\"),\n",
    "    (\"pritamdeka/S-BioBert-snli-multinli-stsb\",    \"icd11_concepts_biobert\"),\n",
    "    (\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\",    \"icd11_concepts_sapbert\"),\n",
    "]\n",
    "\n",
    "QDRANT_HOST = \"localhost\"\n",
    "QDRANT_PORT = 6333\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# --- LOAD JSON FILE ---\n",
    "with open(\"icd11_vector_input.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def generate_points(data):\n",
    "    points = []\n",
    "    for item in data:\n",
    "        # Main concept\n",
    "        points.append((\n",
    "            item[\"concept_name\"],\n",
    "            {\n",
    "                \"concept_name\": item[\"concept_name\"],\n",
    "                **item[\"metadata\"]\n",
    "            }\n",
    "        ))\n",
    "        # Post-coordination\n",
    "        for option in item[\"metadata\"].get(\"postcoordination_options\", []):\n",
    "            points.append((\n",
    "                option[\"title\"],\n",
    "                {\n",
    "                    \"concept_name\": item[\"concept_name\"],\n",
    "                    \"parent_code\": item[\"metadata\"][\"code\"],\n",
    "                    \"postcoordination_title\": option[\"title\"],\n",
    "                    **option\n",
    "                }\n",
    "            ))\n",
    "    return points\n",
    "\n",
    "all_points = generate_points(data)\n",
    "\n",
    "for model_name, collection_name in model_infos:\n",
    "    print(f\"\\n--- Starting for {model_name} -> {collection_name} ---\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)\n",
    "\n",
    "    # 1) Delete collection if it exists\n",
    "    if client.collection_exists(collection_name=collection_name):\n",
    "        client.delete_collection(collection_name=collection_name)\n",
    "\n",
    "    # 2) Create new collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=model.get_sentence_embedding_dimension(),\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 3) Payload indexes for filtering\n",
    "    for field in [\"code\", \"code_type\", \"name_type\", \"is_leaf\"]:\n",
    "        client.create_payload_index(\n",
    "            collection_name=collection_name,\n",
    "            field_name=field,\n",
    "            field_schema=\"keyword\" if field != \"is_leaf\" else \"bool\"\n",
    "        )\n",
    "\n",
    "    # --- Benchmark starts here ---\n",
    "    start = time.time()\n",
    "\n",
    "    batch_texts, batch_payload, batch_ids = [], [], []\n",
    "    for idx, (text, payload) in enumerate(tqdm(all_points, desc=f\"Vectorizing [{collection_name}]\")):\n",
    "        batch_texts.append(text)\n",
    "        batch_payload.append(payload)\n",
    "        batch_ids.append(idx)\n",
    "\n",
    "        if len(batch_texts) >= BATCH_SIZE or idx == len(all_points) - 1:\n",
    "            vectors = model.encode(batch_texts, batch_size=BATCH_SIZE, show_progress_bar=False)\n",
    "            points = [\n",
    "                PointStruct(id=batch_ids[i], vector=vectors[i].tolist(), payload=batch_payload[i])\n",
    "                for i in range(len(batch_texts))\n",
    "            ]\n",
    "            client.upsert(collection_name=collection_name, points=points)\n",
    "            batch_texts, batch_payload, batch_ids = [], [], []\n",
    "\n",
    "    duration = time.time() - start\n",
    "    print(f\"✅ [{collection_name}] Inserted {len(all_points)} points in {duration:.2f} seconds.\")\n",
    "\n",
    "print(\"🏁 Completed processing for all 3 collections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12726392-395a-4ee0-96f2-2b1a59829e5d",
   "metadata": {},
   "source": [
    "## Increasing the number of vectorization points (optional)\n",
    "\n",
    "We can expand our semantic search to include post-coordinated codes, which are clusters that combine stem codes with other stem codes or extension codes. The goal is to check semantic similarity between a given term and these post-coordinated ICD-11 terms.  \n",
    "\n",
    "To achieve this, we concatenate the titles of each stem code with all its related leaf codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf175f5-46b4-4ded-8513-87d2955dca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 1) Load and prepare the SimpleTabulation spreadsheet (official ICD-11 mapping)\n",
    "tab = pd.read_excel(\"./SimpleTabulation-ICD-11-MMS-en.xlsx\", sheet_name=0)\n",
    "\n",
    "# Ensure that all Code values are non-empty strings\n",
    "tab['Code'] = tab['Code'].fillna(\"\").astype(str)\n",
    "\n",
    "# Extract entity_id from the Linearization URI column\n",
    "tab['entity_id'] = tab['Linearization URI'].str.extract(r'/mms/(\\d+)')\n",
    "\n",
    "# Clean leading hyphens from titles\n",
    "tab['title_clean'] = tab['Title'].str.replace(r'^([-]\\s*)+', '', regex=True).str.strip()\n",
    "\n",
    "# Filter only terminal codes (leaf codes)\n",
    "tab = tab[tab['isLeaf'] == True]\n",
    "\n",
    "# Prepare lookup dictionaries for quick access\n",
    "leaf_entity_ids   = set(tab['entity_id'])   # list of all terminal entity_ids\n",
    "entity_to_lin_uri = tab.set_index('entity_id')['Linearization URI'].to_dict()\n",
    "entity_to_source  = tab.set_index('entity_id')['Foundation URI'].to_dict()\n",
    "entity_to_title   = tab.set_index('entity_id')['title_clean'].to_dict()\n",
    "entity_to_code    = tab.set_index('entity_id')['Code'].to_dict()\n",
    "code_to_entity    = {c: eid for eid, c in entity_to_code.items()}  # reverse lookup\n",
    "\n",
    "# 2) Load the extensions/associations file (already with entity_id)\n",
    "lin_ext = pd.read_csv(\"./assets/icd_linear_extensions.csv\", dtype=str).fillna(\"\")\n",
    "\n",
    "# Clean leading hyphens from 'title' and 'extension_title' columns during loading\n",
    "lin_ext['title'] = lin_ext['title'].str.replace(r'^([-]\\s*)+', '', regex=True).str.strip()\n",
    "lin_ext['extension_title'] = lin_ext['extension_title'].str.replace(r'^([-]\\s*)+', '', regex=True).str.strip()\n",
    "\n",
    "# 3) Load the flat hierarchy (for parent-child navigation)\n",
    "flat = pd.read_csv(\"./assets/icd_flat_hierarchy.csv\", dtype=str).fillna(\"\")\n",
    "\n",
    "# Build a map: parent_entity_id → [list of child entity_ids]\n",
    "children_map = flat.groupby('parent_entity_id')['entity_id'].apply(list).to_dict()\n",
    "\n",
    "# Helper function: given an entity_id, return all its terminal (leaf) descendants\n",
    "def get_leaf_descendants(start_eid):\n",
    "    descs = set()\n",
    "    stack = [start_eid]\n",
    "    while stack:\n",
    "        cur = stack.pop()\n",
    "        for ch in children_map.get(cur, []):\n",
    "            if ch in leaf_entity_ids:\n",
    "                descs.add(ch)\n",
    "            else:\n",
    "                stack.append(ch)\n",
    "    return descs\n",
    "\n",
    "# 4) Generate 1:1 combinations (FoundationCode + Extension/AssociationCode)\n",
    "rows = []\n",
    "\n",
    "for _, r in tqdm(lin_ext.iterrows(), total=len(lin_ext), desc=\"Generating final relations\"):\n",
    "    fc_eid = r['entity_id']\n",
    "\n",
    "    # Filter FC: continue only if it is a leaf\n",
    "    if fc_eid not in leaf_entity_ids:\n",
    "        continue\n",
    "        \n",
    "    fc_code  = r['code']\n",
    "    fc_title = r['title'].strip()\n",
    "    lin_uri  = entity_to_lin_uri.get(fc_eid, \"\")\n",
    "\n",
    "    ec_eid = r['extension_entity_id']\n",
    "    if not ec_eid:  # no extension entity → skip\n",
    "        continue\n",
    "\n",
    "    # If extension_code is missing, try to derive it\n",
    "    base_ec_code = r['extension_code'] or entity_to_code.get(ec_eid, \"\")\n",
    "\n",
    "    # Define targets: either the extension itself (if leaf), or all its leaf descendants\n",
    "    targets = [ec_eid] if ec_eid in leaf_entity_ids else list(get_leaf_descendants(ec_eid))\n",
    "\n",
    "    for tgt in targets:\n",
    "        tgt_code  = entity_to_code.get(tgt, \"\")\n",
    "        tgt_title = entity_to_title.get(tgt, \"\")\n",
    "        if not tgt_code:\n",
    "            continue\n",
    "\n",
    "        # Symbol depends on the already resolved target code\n",
    "        symbol = '&' if tgt_code.startswith('X') else '/'\n",
    "        combo_code = f\"{fc_code}{symbol}{tgt_code}\"\n",
    "        combo_name = f\"{fc_title} {tgt_title}\"\n",
    "\n",
    "        rows.append([\n",
    "            combo_name,\n",
    "            combo_code,\n",
    "            \"generated\",\n",
    "            lin_uri,\n",
    "            \"icd11\"\n",
    "        ])\n",
    "\n",
    "# 5) Export the final CSV\n",
    "out = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"name\", \"code\", \"name_type\", \"linearization_uri\", \"source\"]\n",
    ")\n",
    "out.to_csv(\"./icd_combined_relations.csv\", index=False)\n",
    "print(f\"✅ Generated {len(out)} rows in icd_combined_relations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8c061-6945-4234-ae30-ea7fe1814edc",
   "metadata": {},
   "source": [
    "# Working with clinical concepts\n",
    "\n",
    "For our example, we will use the CIEL clinical concept database, which contains curated data, some of which is already mapped to ICD-11. This will serve as a reference for training and as a source of clinical terms for mapping tests.  \n",
    "\n",
    "First, we need to [download the latest version of CIEL](https://www.dropbox.com/scl/fo/wsz5zxc4k96ercqmavhow/AFYq_Y4F6ev8OdTYWHxEnoE?rlkey=bs2flfoevqdkofhl287wpjvuh&st=88g3oeyt&dl=0) and load it into the database.  \n",
    "\n",
    "Please submit a pull request to update this documentation with the database load process or refer directly to the [official documentation](https://openmrs.atlassian.net/wiki/spaces/docs/pages/25470028/Getting+and+Using+the+CIEL+Concept+Dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62d4e6-e263-4a50-ab90-d93f15af9b8a",
   "metadata": {},
   "source": [
    "# AI Agent as ICD-11 Terminologist\n",
    "\n",
    "In addition to semantic search, with the goal of creating an AI terminologist agent capable of mapping clinical concepts to ICD-11 codes—including post-coordinated codes when necessary—we started developing an AI agent for this task.\n",
    "\n",
    "We experimented with training an AI model using curated data.\n",
    "\n",
    "## Creating datasets\n",
    "\n",
    "Several datasets were created for LLM training purposes. The [first dataset](https://huggingface.co/datasets/filipelopesmedbr/CIEL-Clinical-Concepts-to-ICD-11/tree/787a97c5f68a1d70546a58a82a49323abc8ee59d) was simpler, formatted in Alpaca style. The [second, more exhaustive dataset](https://huggingface.co/datasets/filipelopesmedbr/CIEL-Clinical-Concepts-to-ICD-11) included a `<scratchpad>` section for reasoning steps, aiming to reduce hallucinations.\n",
    "\n",
    "## Expanding the set of clinical terms and writing variability\n",
    "\n",
    "We generated paraphrases of clinical concepts marked as Fully Specified Terms (FST) in CIEL to increase variability in the input data to be mapped. This was done using LLM-based paraphrasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13164e-8417-4f01-b1fe-f3cdd0298feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates new paraphrases for all terms in icd11_alpaca_v2.jsonl\n",
    "using the model dmis-lab/meerkat-7b-v1.0.\n",
    "\n",
    "• Filters only records where the \"category\" field contains \"ciel\" \n",
    "  and does NOT contain \"synonym\".\n",
    "• Extracts the code from the first <code> … </code> tag in the \"output\" field.\n",
    "• Extracts the clinical term from the first line of the \"input\" field\n",
    "  after 'Clinical concept to map:'.\n",
    "• Discards paraphrases with a word count greater than 3× the original term.\n",
    "• Cleans prefixes like “1. ”, “• ”, “– ”, etc.\n",
    "• Generates up to 2 distinct paraphrases per term; if only 1 is valid, keeps just 1.\n",
    "• Outputs a file called paraphrased_dataset.jsonl where each line is a\n",
    "  JSON object with: {\"code\", \"input\", \"paraphrase\"}.\n",
    "\"\"\"\n",
    "\n",
    "import json, re, random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ── 0. CONFIGURATION ─────────────────────────────────────────────────────\n",
    "SEED         = 1234\n",
    "random.seed(SEED)\n",
    "\n",
    "SRC_FILE     = Path(\"./icd11_alpaca_v2.jsonl\")\n",
    "OUT_FILE     = Path(\"./paraphrased_dataset.jsonl\")\n",
    "MODEL_ID     = \"dmis-lab/meerkat-7b-v1.0\"\n",
    "MAX_NEW      = 96\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ── 1. LOAD THE MODEL ────────────────────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model     = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ── 2. HELPER FUNCTIONS ──────────────────────────────────────────────────\n",
    "prompt_tpl = (\n",
    "    \"Paraphrase the following medical term, preserving the meaning and \"\n",
    "    \"qualifiers:\\n{term}\\nParaphrases:\"\n",
    ")\n",
    "\n",
    "def clean_prefix(text: str) -> str:\n",
    "    \"\"\"Remove prefixes like '1. ', '• ', '- ' at the start of the string.\"\"\"\n",
    "    return re.sub(r\"^\\s*(\\d+[\\.\\)]|[-•])\\s*\", \"\", text).strip()\n",
    "\n",
    "def generate_paraphrases(term: str, beams: int = 5, ret: int = 4) -> list[str]:\n",
    "    \"\"\"Generate up to two valid paraphrases for the given term.\"\"\"\n",
    "    prompt = prompt_tpl.format(term=term)\n",
    "    ids    = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    outs   = model.generate(\n",
    "        **ids,\n",
    "        max_new_tokens=MAX_NEW,\n",
    "        num_beams=beams,\n",
    "        num_return_sequences=ret,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    src_words = term.split()\n",
    "    max_len   = 3 * len(src_words)\n",
    "\n",
    "    seen, final = set(), []\n",
    "    for dec in tokenizer.batch_decode(outs, skip_special_tokens=True):\n",
    "        text = dec.replace(prompt, \"\").strip().split(\"\\n\")[0]\n",
    "        text = clean_prefix(text).strip(\" .\")\n",
    "        if (\n",
    "            not text\n",
    "            or text.lower() == term.lower()\n",
    "            or len(text.split()) > max_len\n",
    "            or text in seen\n",
    "        ):\n",
    "            continue\n",
    "        seen.add(text)\n",
    "        final.append(text)\n",
    "        if len(final) == 2:\n",
    "            break\n",
    "\n",
    "    return final\n",
    "\n",
    "def extract_code(output_field: str) -> str | None:\n",
    "    match = re.search(r\"<code>([^<]+)</code>\", output_field)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "def extract_term(input_field: str) -> str:\n",
    "    match = re.search(r\"<input>(.*?)</input>\", input_field)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    # Fallback for older input format\n",
    "    return input_field.split(\"Clinical concept to map:\")[1].splitlines()[0].strip()\n",
    "\n",
    "# ── 3. FILTER VALID RECORDS ──────────────────────────────────────────────\n",
    "with SRC_FILE.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    records = [\n",
    "        json.loads(line) for line in f\n",
    "        if \"ciel\" in line and \"synonym\" not in line\n",
    "    ]\n",
    "\n",
    "# ── 4. MAIN LOOP WITH PROGRESS BAR ───────────────────────────────────────\n",
    "with OUT_FILE.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "    for rec in tqdm(records, desc=\"Generating paraphrases\"):\n",
    "        code = extract_code(rec.get(\"output\", \"\"))\n",
    "        if not code:\n",
    "            continue\n",
    "        try:\n",
    "            term = extract_term(rec.get(\"input\", \"\"))\n",
    "        except:\n",
    "            continue\n",
    "        paras = generate_paraphrases(term)\n",
    "        for p in paras:\n",
    "            json.dump({\"code\": code, \"input\": term, \"paraphrase\": p}, f_out, ensure_ascii=False)\n",
    "            f_out.write(\"\\n\")\n",
    "\n",
    "print(f\"\\n✅ File saved at: {OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83097d3a-709b-480a-bab2-05c1b4f72dcd",
   "metadata": {},
   "source": [
    "## LLM Training\n",
    "\n",
    "The training process improved the LLM's ability to understand ICD-11 coding patterns. However, the accuracy of the results was quite poor when the models were used in isolation without any RAG (Retrieval-Augmented Generation) methods.  \n",
    "\n",
    "The models often either hallucinated answers or produced too many mappings to extension codes, which should not be mapped in most cases.  \n",
    "\n",
    "The [repository includes two models](https://huggingface.co/filipelopesmedbr/icd11-llm-ministral-8b). The second version shows better reasoning capacity but also exhibits more hallucination and often fails to produce a concrete answer after long reasoning paths."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
