{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454f7a99-9470-4692-bbe2-d991f5069153",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf729c62-57db-4c15-9bee-856da40bf9c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.11/site-packages (2.0.22)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.1)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, pymysql, dotenv\n",
      "Successfully installed dotenv-0.9.9 pymysql-1.1.1 python-dotenv-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql sqlalchemy pandas dotenv tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fad73790-c18e-4565-b825-fdc6d02c1736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating database connections...\n",
      "‚úÖ Database connections successfully established.\n",
      "<sqlalchemy.engine.cursor.CursorResult object at 0x7b9e4ebf7380>\n",
      "‚úÖ Successfully connected to snapshot_20250405:3306: snapshot_20250405\n",
      "<sqlalchemy.engine.cursor.CursorResult object at 0x7b9e4ebf73f0>\n",
      "‚úÖ Successfully connected to openmrs:3306: openmrs\n",
      "<sqlalchemy.engine.cursor.CursorResult object at 0x7b9e4ebf7150>\n",
      "‚úÖ Successfully connected to analytics:3306: analytics\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine, text, event\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def validate_env_vars(required: dict, context: str = \"\"):\n",
    "    for var_name, value in required.items():\n",
    "        if not value:\n",
    "            raise ValueError(f\"[{context}] The environment variable {var_name} is not defined!\")\n",
    "\n",
    "# Production environment variables\n",
    "PRODUCTION_DB_USER = os.getenv(\"CIEL_PROD_DB_USER\")\n",
    "PRODUCTION_DB_PASSWORD = os.getenv(\"CIEL_PROD_DB_PASS\")\n",
    "PRODUCTION_DB_HOST = os.getenv(\"CIEL_PROD_DB_HOST\")\n",
    "PRODUCTION_DB_PORT = os.getenv(\"CIEL_PROD_DB_PORT\")\n",
    "PRODUCTION_DB_NAME = os.getenv(\"CIEL_PROD_DB_NAME\")\n",
    "PRODUCTION_ANALYTICS_DB_NAME = os.getenv(\"CIEL_PROD_ANALYTICS_DB_NAME\")\n",
    "\n",
    "validate_env_vars({\n",
    "    \"CIEL_PROD_DB_USER\": PRODUCTION_DB_USER,\n",
    "    \"CIEL_PROD_DB_PASS\": PRODUCTION_DB_PASSWORD,\n",
    "    \"CIEL_PROD_DB_HOST\": PRODUCTION_DB_HOST,\n",
    "    \"CIEL_PROD_DB_PORT\": PRODUCTION_DB_PORT,\n",
    "    \"CIEL_PROD_DB_NAME\": PRODUCTION_DB_NAME,\n",
    "    \"CIEL_PROD_ANALYTICS_DB_NAME\": PRODUCTION_ANALYTICS_DB_NAME,\n",
    "}, context=\"PRODUCTION\")\n",
    "\n",
    "PRODUCTION_DATABASE_URL = f\"mysql+pymysql://{PRODUCTION_DB_USER}:{PRODUCTION_DB_PASSWORD}@{PRODUCTION_DB_HOST}:{PRODUCTION_DB_PORT}/{PRODUCTION_DB_NAME}\"\n",
    "PRODUCTION_ANALYTICS_DATABASE_URL = f\"mysql+pymysql://{PRODUCTION_DB_USER}:{PRODUCTION_DB_PASSWORD}@{PRODUCTION_DB_HOST}:{PRODUCTION_DB_PORT}/{PRODUCTION_ANALYTICS_DB_NAME}\"\n",
    "\n",
    "# Development environment variables (corrected)\n",
    "DEVELOPMENT_DB_USER = os.getenv(\"CIEL_DEV_DB_USER\")\n",
    "DEVELOPMENT_DB_PASSWORD = os.getenv(\"CIEL_DEV_DB_PASS\")\n",
    "DEVELOPMENT_DB_HOST = os.getenv(\"CIEL_DEV_DB_HOST\")\n",
    "DEVELOPMENT_DB_PORT = os.getenv(\"CIEL_DEV_DB_PORT\")\n",
    "DEVELOPMENT_DB_NAME = os.getenv(\"CIEL_DEV_DB_NAME\")\n",
    "\n",
    "validate_env_vars({\n",
    "    \"CIEL_DEV_DB_USER\": DEVELOPMENT_DB_USER,\n",
    "    \"CIEL_DEV_DB_PASS\": DEVELOPMENT_DB_PASSWORD,\n",
    "    \"CIEL_DEV_DB_HOST\": DEVELOPMENT_DB_HOST,\n",
    "    \"CIEL_DEV_DB_PORT\": DEVELOPMENT_DB_PORT,\n",
    "    \"CIEL_DEV_DB_NAME\": DEVELOPMENT_DB_NAME,\n",
    "}, context=\"DEVELOPMENT\")\n",
    "\n",
    "DEVELOPMENT_DATABASE_URL = f\"mysql+pymysql://{DEVELOPMENT_DB_USER}:{DEVELOPMENT_DB_PASSWORD}@{DEVELOPMENT_DB_HOST}:{DEVELOPMENT_DB_PORT}/{DEVELOPMENT_DB_NAME}\"\n",
    "\n",
    "# Create SQLAlchemy engines\n",
    "try:\n",
    "    print(\"üîÑ Creating database connections...\")\n",
    "    production_engine = create_engine(PRODUCTION_DATABASE_URL)\n",
    "    production_analytics_engine = create_engine(PRODUCTION_ANALYTICS_DATABASE_URL)\n",
    "    development_engine = create_engine(DEVELOPMENT_DATABASE_URL, pool_pre_ping=True)\n",
    "    print(\"‚úÖ Database connections successfully established.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating database engines: {e}\")\n",
    "    raise\n",
    "\n",
    "# Function to test database connectivity\n",
    "def test_connection(engine, db_name, db_port):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(text(\"SELECT DATABASE();\"))\n",
    "            print(result)\n",
    "            print(f\"‚úÖ Successfully connected to {db_name}:{db_port}: {result.fetchone()[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to {db_name}: {e}\")\n",
    "\n",
    "# Test all connections\n",
    "test_connection(development_engine, DEVELOPMENT_DB_NAME, DEVELOPMENT_DB_PORT)\n",
    "test_connection(production_engine, PRODUCTION_DB_NAME, PRODUCTION_DB_PORT)\n",
    "test_connection(production_analytics_engine, PRODUCTION_ANALYTICS_DB_NAME, PRODUCTION_DB_PORT)\n",
    "\n",
    "# Event listener to block write operations\n",
    "@event.listens_for(production_engine, \"before_cursor_execute\")\n",
    "def prevent_writes(conn, cursor, statement, parameters, context, executemany):\n",
    "    lowered = statement.strip().lower()\n",
    "    if lowered.startswith((\"insert\", \"update\", \"delete\", \"create\", \"drop\", \"alter\", \"truncate\", \"replace\")):\n",
    "        raise Exception(f\"‚ùå [READ-ONLY] Write operation blocked: `{statement.split()[0].upper()}`\")\n",
    "\n",
    "# Create alias\n",
    "engine = production_analytics_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add8dff-5708-4596-9211-dbf2fafa1ac1",
   "metadata": {},
   "source": [
    "## Generate Dataset V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebef3ac-ddc7-4ccd-beb6-1e980caa0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# üìÇ Carregar vari√°veis de ambiente\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# üì¶ Ler apenas o necess√°rio para conectar no banco analytics\n",
    "DB_USER = os.getenv(\"CIEL_PROD_DB_USER\")\n",
    "DB_PASS = os.getenv(\"CIEL_PROD_DB_PASS\")\n",
    "DB_HOST = os.getenv(\"CIEL_PROD_DB_HOST\")\n",
    "DB_PORT = os.getenv(\"CIEL_PROD_DB_PORT\")\n",
    "DB_NAME = os.getenv(\"CIEL_PROD_ANALYTICS_DB_NAME\")\n",
    "\n",
    "# üõ°Ô∏è Valida√ß√£o b√°sica\n",
    "for var_name, value in {\n",
    "    \"CIEL_PROD_DB_USER\": DB_USER,\n",
    "    \"CIEL_PROD_DB_PASS\": DB_PASS,\n",
    "    \"CIEL_PROD_DB_HOST\": DB_HOST,\n",
    "    \"CIEL_PROD_DB_PORT\": DB_PORT,\n",
    "    \"CIEL_PROD_ANALYTICS_DB_NAME\": DB_NAME,\n",
    "}.items():\n",
    "    if not value:\n",
    "        raise ValueError(f\"Environment variable {var_name} is missing!\")\n",
    "\n",
    "# üõú Criar conex√£o SQLAlchemy\n",
    "DATABASE_URL = f\"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "engine = create_engine(DATABASE_URL, pool_pre_ping=True)\n",
    "\n",
    "# ‚úÖ Testar conex√£o\n",
    "def test_connection(engine):\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT DATABASE();\")).scalar()\n",
    "        print(f\"‚úÖ Connected to database: {result}\")\n",
    "\n",
    "test_connection(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fd89d-3930-4b5c-89ce-9c47adae007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# üìÇ 1. Carregar vari√°veis de ambiente e conectar no banco\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DB_USER = os.getenv(\"CIEL_PROD_DB_USER\")\n",
    "DB_PASS = os.getenv(\"CIEL_PROD_DB_PASS\")\n",
    "DB_HOST = os.getenv(\"CIEL_PROD_DB_HOST\")\n",
    "DB_PORT = os.getenv(\"CIEL_PROD_DB_PORT\")\n",
    "DB_NAME = os.getenv(\"CIEL_PROD_ANALYTICS_DB_NAME\")\n",
    "\n",
    "DATABASE_URL = f\"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(DATABASE_URL, pool_pre_ping=True)\n",
    "\n",
    "# üéØ 2. Definir instru√ß√µes poss√≠veis\n",
    "instructions_stem = [\n",
    "    \"Provide the ICD-11 code for the clinical concept. Include extensions or cluster codes if required for full specificity.\",\n",
    "    \"Find the ICD-11 code corresponding to the clinical concept. Use extensions or clusters if necessary.\",\n",
    "    \"Map the clinical concept to its ICD-11 code. Add extensions or cluster codes if needed.\",\n",
    "    \"Assign the ICD-11 code to the following concept, considering extensions or cluster codes where appropriate.\"\n",
    "]\n",
    "\n",
    "instructions_extension = [\n",
    "    \"Provide the ICD-11 extension code corresponding to the specified extension concept.\",\n",
    "    \"Identify the ICD-11 extension code for the following extension concept.\",\n",
    "    \"Find the corresponding ICD-11 extension code for this modifier.\"\n",
    "]\n",
    "\n",
    "# üì¶ 3. Inicializar dataset\n",
    "dataset = []\n",
    "\n",
    "# üìö 4. Ler e processar SimpleTabulation-ICD-11-MMS-en.xlsx\n",
    "file_path = '../assets/SimpleTabulation-ICD-11-MMS-en.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df = df[\n",
    "    (df['isLeaf'] == True) &\n",
    "    (df['Code'].notnull()) &\n",
    "    (df['ClassKind'] == 'category')\n",
    "]\n",
    "\n",
    "print(f\"üìÑ Processando {len(df)} linhas da planilha...\")\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    code = row['Code']\n",
    "    title = str(row['Title']).lstrip('-‚Äì‚Äî ').strip()\n",
    "    chapter = row['ChapterNo']\n",
    "\n",
    "    if code.startswith('X') and chapter == 'X':\n",
    "        category = 'source:icdapi>extension'\n",
    "        instruction = random.choice(instructions_extension)\n",
    "    else:\n",
    "        category = 'source:icdapi>same-as+stem'\n",
    "        instruction = random.choice(instructions_stem)\n",
    "\n",
    "    entry = {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": title,\n",
    "        \"output\": f\"SAME-AS {code}\",\n",
    "        \"category\": category\n",
    "    }\n",
    "\n",
    "    dataset.append(entry)\n",
    "\n",
    "# üõú 5. Ler e processar dados da view vw_ciel_diagnosis_n_findings_to_icd11\n",
    "query = text(\"\"\"\n",
    "    SELECT concept_id, fsn, map_type, icd11_code\n",
    "    FROM analytics.vw_ciel_diagnosis_n_findings_to_icd11\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    results = conn.execute(query).mappings().all()\n",
    "\n",
    "print(f\"üõ¢Ô∏è Processando {len(results)} linhas da view...\")\n",
    "\n",
    "for row in tqdm(results):\n",
    "    concept_id = row['concept_id']\n",
    "    fsn = row['fsn']\n",
    "    map_type = row['map_type']\n",
    "    icd11_code = row['icd11_code']\n",
    "\n",
    "    if icd11_code.startswith('X'):\n",
    "        continue\n",
    "\n",
    "    category = None\n",
    "\n",
    "    if '&' not in icd11_code and '/' not in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+stem'\n",
    "        elif map_type == \"NARROWER-THAN\":\n",
    "            category = 'source:ciel>narrower-than+stem'\n",
    "        elif map_type == \"BROADER-THAN\":\n",
    "            category = 'source:ciel>broader-than+stem'\n",
    "\n",
    "    elif icd11_code.count('&') == 1 and '/' not in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+stem+extension'\n",
    "        elif map_type == \"NARROWER-THAN\":\n",
    "            category = 'source:ciel>narrower-than+stem+extension'\n",
    "        elif map_type == \"BROADER-THAN\":\n",
    "            category = 'source:ciel>broader-than+stem+extension'\n",
    "\n",
    "    elif icd11_code.count('&') > 1 and '/' not in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+stem+extensions'\n",
    "        elif map_type == \"NARROWER-THAN\":\n",
    "            category = 'source:ciel>narrower-than+stem+extensions'\n",
    "        elif map_type == \"BROADER-THAN\":\n",
    "            category = 'source:ciel>broader-than+stem+extensions'\n",
    "\n",
    "    elif '&' not in icd11_code and '/' in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+cluster'\n",
    "\n",
    "    elif '&' in icd11_code and '/' in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+cluster+extension'\n",
    "\n",
    "    if category is None:\n",
    "        continue\n",
    "\n",
    "    entry = {\n",
    "        \"instruction\": random.choice(instructions_stem),\n",
    "        \"input\": fsn,\n",
    "        \"output\": f\"{map_type} {icd11_code}\",\n",
    "        \"category\": category\n",
    "    }\n",
    "\n",
    "    dataset.append(entry)\n",
    "\n",
    "# üíæ 6. Exportar JSONL unificado\n",
    "output_path = '../assets/ciel-icd11-unified.jsonl'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for item in dataset:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"‚úÖ Dataset final salvo em {output_path} com {len(dataset)} exemplos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816b0cf-9433-4d7d-aaa8-a02030e364a7",
   "metadata": {},
   "source": [
    "### Testing (QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4e9008e6-fb53-4b72-a961-bf12406bb327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Total de linhas: 51838\n",
      "\n",
      "üìö Categorias √∫nicas: 13\n",
      "  ‚Ä¢ source:icdapi>same-as+stem: 16019 exemplos\n",
      "  ‚Ä¢ source:icdapi>extension: 14947 exemplos\n",
      "  ‚Ä¢ source:ciel>broader-than+stem: 1136 exemplos\n",
      "  ‚Ä¢ source:ciel>same-as+stem: 4965 exemplos\n",
      "  ‚Ä¢ source:ciel>narrower-than+stem: 8578 exemplos\n",
      "  ‚Ä¢ source:ciel>narrower-than+stem+extension: 2910 exemplos\n",
      "  ‚Ä¢ source:ciel>same-as+stem+extension: 2283 exemplos\n",
      "  ‚Ä¢ source:ciel>broader-than+stem+extension: 337 exemplos\n",
      "  ‚Ä¢ source:ciel>same-as+cluster: 280 exemplos\n",
      "  ‚Ä¢ source:ciel>same-as+cluster+extension: 115 exemplos\n",
      "  ‚Ä¢ source:ciel>narrower-than+stem+extensions: 62 exemplos\n",
      "  ‚Ä¢ source:ciel>same-as+stem+extensions: 198 exemplos\n",
      "  ‚Ä¢ source:ciel>broader-than+stem+extensions: 8 exemplos\n",
      "\n",
      "üìù Instru√ß√µes √∫nicas: 7\n",
      "  ‚Ä¢ Map the clinical concept to its ICD-11 code. Add e... (9302 exemplos)\n",
      "  ‚Ä¢ Assign the ICD-11 code to the following concept, c... (9238 exemplos)\n",
      "  ‚Ä¢ Find the ICD-11 code corresponding to the clinical... (9203 exemplos)\n",
      "  ‚Ä¢ Provide the ICD-11 code for the clinical concept. ... (9148 exemplos)\n",
      "  ‚Ä¢ Identify the ICD-11 extension code for the followi... (5066 exemplos)\n",
      "  ‚Ä¢ Find the corresponding ICD-11 extension code for t... (4958 exemplos)\n",
      "  ‚Ä¢ Provide the ICD-11 extension code corresponding to... (4923 exemplos)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# üìÇ Caminho para o seu arquivo\n",
    "file_path = '../assets/ciel-icd11-unified.jsonl'\n",
    "\n",
    "# üì¶ Carregar dataset\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# üß† Converter para DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# üìä Contar categorias\n",
    "category_counts = Counter(df['category'])\n",
    "instruction_counts = Counter(df['instruction'])\n",
    "\n",
    "# üìã Mostrar resumo\n",
    "print(f\"üî¢ Total de linhas: {len(df)}\")\n",
    "print(f\"\\nüìö Categorias √∫nicas: {len(category_counts)}\")\n",
    "for cat, count in category_counts.items():\n",
    "    print(f\"  ‚Ä¢ {cat}: {count} exemplos\")\n",
    "\n",
    "print(f\"\\nüìù Instru√ß√µes √∫nicas: {len(instruction_counts)}\")\n",
    "for instr, count in instruction_counts.items():\n",
    "    print(f\"  ‚Ä¢ {instr[:50]}... ({count} exemplos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c51fc-7b27-424c-a4e5-95f200891ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# üìÇ Carregar o dataset\n",
    "input_path = '../assets/ciel-icd11-unified.jsonl'\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# üóÇÔ∏è Separar base icdapi e ciel\n",
    "base_data = []\n",
    "ciel_data = defaultdict(list)  # agora agrupamos ciel por categoria\n",
    "\n",
    "for item in data:\n",
    "    if item['category'].startswith('source:icdapi'):\n",
    "        base_data.append(item)\n",
    "    else:\n",
    "        ciel_data[item['category']].append(item)\n",
    "\n",
    "# üì¶ Preparar listas de treino e valida√ß√£o\n",
    "train_data = base_data.copy()\n",
    "val_data = []\n",
    "\n",
    "# üéØ Balancear cada categoria CIEL 9:1\n",
    "for category, items in ciel_data.items():\n",
    "    random.shuffle(items)\n",
    "    split_idx = int(len(items) * 0.9)\n",
    "    train_data.extend(items[:split_idx])\n",
    "    val_data.extend(items[split_idx:])\n",
    "\n",
    "# üíæ Salvar\n",
    "with open('../assets/ciel-icd11-train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../assets/ciel-icd11-validation.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"‚úÖ Treino: {len(train_data)} exemplos\")\n",
    "print(f\"‚úÖ Valida√ß√£o: {len(val_data)} exemplos\")\n",
    "print(f\"üîµ Base icdapi no treino: {len(base_data)} exemplos\")\n",
    "for cat, items in ciel_data.items():\n",
    "    print(f\"üìö Categoria {cat}: {len(items)} exemplos (9:1 aplicado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac2151b-88cf-4f54-af1f-84fe42533487",
   "metadata": {},
   "source": [
    "axolotl.yml\n",
    "```yml\n",
    "base_model: mistralai/Ministral-8B-Instruct-2410\n",
    "model_type: mistral\n",
    "tokenizer_type: mistral\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: true\n",
    "bnb_4bit_compute_dtype: bfloat16\n",
    "bnb_4bit_use_double_quant: true\n",
    "bnb_4bit_quant_type: nf4\n",
    "trust_remote_code: true\n",
    "\n",
    "datasets:\n",
    "  - path: ../assets/ciel-icd11.jsonl\n",
    "    type: completion\n",
    "    field_instruction: instruction\n",
    "    field_input: input\n",
    "    field_output: output\n",
    "    field_category: category\n",
    "    eval_split: 0.1\n",
    "\n",
    "dataset_prepared_path: last_run_prepared\n",
    "val_set_size: 0.1\n",
    "output_dir: ./axolotl-out-ministral-8b-ciel\n",
    "logging_steps: 20\n",
    "save_steps: 200\n",
    "save_total_limit: 2\n",
    "early_stopping: true\n",
    "early_stopping_patience: 3\n",
    "\n",
    "adapter: lora\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "target_modules:\n",
    "  - q_proj\n",
    "  - k_proj\n",
    "  - v_proj\n",
    "  - o_proj\n",
    "  - gate_proj\n",
    "  - up_proj\n",
    "  - down_proj\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "train_on_inputs: false\n",
    "group_by_length: true\n",
    "\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 2e-4\n",
    "warmup_steps: 50\n",
    "\n",
    "gradient_checkpointing: true\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 16\n",
    "num_epochs: 3\n",
    "\n",
    "weight_decay: 0.01\n",
    "bf16: true\n",
    "tf32: true\n",
    "flash_attention: true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec99684-1a6f-4a5e-8e46-a45e0fcad1c7",
   "metadata": {},
   "source": [
    "## Generate Dataset V2\n",
    "\n",
    "- [ ] Create Source database - create auxiliar tables on postgres from ICD API\n",
    "- [ ] Create Source database - Create vector database in Qdrant\n",
    "- [ ] Prompt Enginniering - Add context to input using top-7 retrieval  \n",
    "  1. Query vector database for candidate matches  \n",
    "  2. Check cosine distance of matches ‚Äî if greater than 0.3, post-coordination is usually needed (to study more about it)  \n",
    "  3. Evaluate metadata: for each detected stem code, filter top-3 matching codes from its recommended post-coordination options  \n",
    "- [ ] Prompt Enginniering - Add reasoning line to the output  \n",
    "- [ ] Weightning - Add more weight to FSN terms and non-SAME-AS entries to balance the dataset samples\n",
    "- [ ] Adicionar par√°frases com metade do peso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a83a46-4852-4530-9969-d8a388d30647",
   "metadata": {},
   "source": [
    "### Preparing JSON for sending to vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df65415-5738-4c06-b7e1-d20dd58cb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 1. Carregar dataframes\n",
    "# icd11_df j√° deve estar carregado com colunas: id, title, code, is_leaf, entity_id_residual\n",
    "\n",
    "print(\"Carregando icd11_mms_en_name...\")\n",
    "names_df = pd.read_sql(\n",
    "    \"SELECT icd11_mms_en_id, name, name_type FROM analytics.icd11_mms_en_name\",\n",
    "    engine\n",
    ")\n",
    "\n",
    "print(\"Carregando icd11_mms_en_postcoordination...\")\n",
    "postcoord_df = pd.read_sql(\"\"\"\n",
    "    SELECT icd11_mms_en_id, code, code_type, title\n",
    "    FROM analytics.icd11_mms_en_postcoordination\n",
    "\"\"\", engine)\n",
    "\n",
    "# 2. Juntar is_leaf, entity_id_residual para cada code\n",
    "print(\"Criando dataset postcoord_with_more...\")\n",
    "postcoord_with_more = (\n",
    "    postcoord_df\n",
    "    .merge(\n",
    "        icd11_df[['code', 'is_leaf', 'entity_id_residual']],\n",
    "        on='code',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Agrupar postcoordina√ß√£o por stem_id para acesso r√°pido\n",
    "postcoord_grouped = postcoord_with_more.groupby('icd11_mms_en_id')\n",
    "\n",
    "# 4. Montar lista de objetos JSON\n",
    "vector_input = []\n",
    "for _, row in tqdm(icd11_df.iterrows(), total=len(icd11_df), desc=\"Gerando JSON\"):\n",
    "    cid = row['id']\n",
    "    title = row['title']\n",
    "    code = row['code']\n",
    "    is_leaf = bool(row['is_leaf'])\n",
    "    entity_id_residual = row['entity_id_residual']\n",
    "    \n",
    "    if not code:\n",
    "        code_type = \"foundation\"\n",
    "    elif str(code).startswith(\"X\"):\n",
    "        code_type = \"extension\"\n",
    "    else:\n",
    "        code_type = \"stem\"\n",
    "\n",
    "    metadata = {\n",
    "        \"code\": code,\n",
    "        \"entity_id_residual\": entity_id_residual,\n",
    "        \"code_type\": code_type,\n",
    "        \"name_type\": \"fsn\",\n",
    "        \"is_leaf\": is_leaf\n",
    "    }\n",
    "\n",
    "    opts = postcoord_grouped.get_group(cid) if cid in postcoord_grouped.groups else pd.DataFrame()\n",
    "\n",
    "    metadata[\"leaf_postcoordination_options\"] = [\n",
    "        {\n",
    "            \"code\": opt_row['code'],\n",
    "            \"code_type\": opt_row['code_type'],\n",
    "            \"title\": opt_row['title'],\n",
    "            \"is_leaf\": bool(opt_row['is_leaf']),\n",
    "            \"entity_id_residual\": opt_row['entity_id_residual']\n",
    "        }\n",
    "        for _, opt_row in opts.iterrows() if pd.notnull(opt_row['code'])\n",
    "    ]\n",
    "\n",
    "    vector_input.append({\n",
    "        \"concept_name\": title,\n",
    "        \"metadata\": metadata\n",
    "    })\n",
    "\n",
    "# 5. Salvar em JSON\n",
    "with open(\"icd11_vector_input.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vector_input, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Gerado icd11_vector_input.json com {len(vector_input)} conceitos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6549453-44fb-4075-8e43-e486c633b993",
   "metadata": {},
   "source": [
    "### Generatins Preliminar dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a50254b-d7d9-4cc2-a720-e5246be1f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalogando nomes...\n",
      "Catalogando op√ß√µes de p√≥s coordena√ß√£o...\n",
      "Catalogando parentesco...\n",
      "Catalogando blocos/categorias...\n",
      "Adicionando dados ao dataset de p√≥s coordena√ß√£o...\n",
      "Catalogando criando mapa de sin√¥nimos...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. Carregar os cap√≠tulos ---\n",
    "icd11_chapters = {\n",
    "    \"01\": \"Certain infectious or parasitic diseases\",\n",
    "    \"02\": \"Neoplasms\",\n",
    "    \"03\": \"Diseases of the blood or blood-forming organs\",\n",
    "    \"04\": \"Diseases of the immune system\",\n",
    "    \"05\": \"Endocrine, nutritional or metabolic diseases\",\n",
    "    \"06\": \"Mental, behavioural or neurodevelopmental disorders\",\n",
    "    \"07\": \"Sleep-wake disorders\",\n",
    "    \"08\": \"Diseases of the nervous system\",\n",
    "    \"09\": \"Diseases of the visual system\",\n",
    "    \"10\": \"Diseases of the ear or mastoid process\",\n",
    "    \"11\": \"Diseases of the circulatory system\",\n",
    "    \"12\": \"Diseases of the respiratory system\",\n",
    "    \"13\": \"Diseases of the digestive system\",\n",
    "    \"14\": \"Diseases of the skin\",\n",
    "    \"15\": \"Diseases of the musculoskeletal system or connective tissue\",\n",
    "    \"16\": \"Diseases of the genitourinary system\",\n",
    "    \"17\": \"Conditions related to sexual health\",\n",
    "    \"18\": \"Pregnancy, childbirth or the puerperium\",\n",
    "    \"19\": \"Certain conditions originating in the perinatal period\",\n",
    "    \"20\": \"Developmental anomalies\",\n",
    "    \"21\": \"Symptoms, signs or clinical findings, not elsewhere classified\",\n",
    "    \"22\": \"Injury, poisoning or certain other consequences of external causes\",\n",
    "    \"23\": \"External causes of morbidity or mortality\",\n",
    "    \"24\": \"Factors influencing health status or contact with health services\",\n",
    "    \"25\": \"Codes for special purposes\",\n",
    "    \"26\": \"Supplementary Chapter Traditional Medicine Conditions\",\n",
    "    \"X\":  \"Extension Codes\"\n",
    "}\n",
    "\n",
    "# --- 2. Carregar DataFrames necess√°rios ---\n",
    "print(\"Catalogando nomes...\")\n",
    "names_df = pd.read_sql(\n",
    "    \"SELECT icd11_mms_en_id, name, name_type FROM analytics.icd11_mms_en_name\",\n",
    "    engine\n",
    ")\n",
    "\n",
    "print(\"Catalogando op√ß√µes de p√≥s coordena√ß√£o...\")\n",
    "postcoord_df = pd.read_sql(\"\"\"\n",
    "    SELECT icd11_mms_en_id, code, code_type, title, required\n",
    "    FROM analytics.icd11_mms_en_postcoordination\n",
    "\"\"\", engine)\n",
    "\n",
    "print(\"Catalogando parentesco...\")\n",
    "# flat_hierarchy: Para buscar parents em cadeia\n",
    "flat = pd.read_csv(\"../assets/icd_flat_hierarchy.csv\", dtype=str).fillna(\"\")\n",
    "flat = flat[['entity_id', 'code', 'title', 'parent_entity_id']]  # garantir colunas corretas\n",
    "\n",
    "print(\"Catalogando blocos/categorias...\")\n",
    "# --- 3. Preprocessar mapeamentos auxiliares para blocks e parents ---\n",
    "# Lookup para blocks a partir de icd11_df\n",
    "block_id_to_title = dict(zip(icd11_df['block_id'], icd11_df['title']))\n",
    "\n",
    "# Para blocks (groupings): extrai todos os groupingN de cada linha\n",
    "def get_blocks(row):\n",
    "    blocks = []\n",
    "    for n in range(1, 6):\n",
    "        grp_code = row.get(f'grouping{n}')\n",
    "        if pd.notnull(grp_code) and grp_code:\n",
    "            # Pega t√≠tulo do bloco (pode ser nulo)\n",
    "            grp_title = icd11_df.loc[icd11_df['block_id'] == grp_code, 'title'].values\n",
    "            blocks.append({\n",
    "                \"code\": grp_code,\n",
    "                \"title\": grp_title[0] if len(grp_title) > 0 else None\n",
    "            })\n",
    "    return blocks\n",
    "\n",
    "def get_entity_id_base(entity_id_residual):\n",
    "    \"\"\"\n",
    "    Retorna apenas a parte num√©rica antes de '/' em entity_id_residual.\n",
    "    Ex: '12345678/other' -> '12345678', '9876543' -> '9876543'\n",
    "    \"\"\"\n",
    "    if pd.isnull(entity_id_residual):\n",
    "        return \"\"\n",
    "    return str(entity_id_residual).split('/')[0]\n",
    "    \n",
    "# Para parents (ancestrais): busca recursivamente em flat_hierarchy\n",
    "def get_parents(entity_id_residual):\n",
    "    \"\"\"\n",
    "    Busca ancestrais na flat hierarchy com base no entity_id_residual base.\n",
    "    Retorna uma lista de dicion√°rios: code, title, entity_id_residual\n",
    "    \"\"\"\n",
    "    parents = []\n",
    "    eid = str(entity_id_residual).split('/')[0]\n",
    "    while True:\n",
    "        row = flat.loc[flat['entity_id'] == eid]\n",
    "        if row.empty:\n",
    "            break\n",
    "        parent_id = row['parent_entity_id'].values[0]\n",
    "        parent_code = row['code'].values[0]\n",
    "        parent_title = row['title'].values[0]\n",
    "        if parent_id and parent_id != '' and parent_id != eid:\n",
    "            # Busca info do pai direto (parent_id)\n",
    "            parent_row = flat.loc[flat['entity_id'] == parent_id]\n",
    "            if not parent_row.empty:\n",
    "                parents.append({\n",
    "                    \"code\": parent_row['code'].values[0],\n",
    "                    \"title\": parent_row['title'].values[0],\n",
    "                    \"entity_id_residual\": parent_id\n",
    "                })\n",
    "            eid = parent_id  # sobe para o pr√≥ximo ancestral\n",
    "        else:\n",
    "            break\n",
    "    return parents\n",
    "\n",
    "print(\"Adicionando dados ao dataset de p√≥s coordena√ß√£o...\")\n",
    "# --- 4. Agrupamento de p√≥s-coordena√ß√£o por conceito ---\n",
    "postcoord_with_more = (\n",
    "    postcoord_df\n",
    "    .merge(\n",
    "        icd11_df[['code', 'is_leaf', 'entity_id_residual']],\n",
    "        on='code',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "postcoord_grouped = postcoord_with_more.groupby('icd11_mms_en_id')\n",
    "\n",
    "# --- 5. Names: map id ‚Üí synonyms (lista) ---\n",
    "print(\"Catalogando criando mapa de sin√¥nimos...\")\n",
    "synonyms_map = names_df[names_df['name_type'] == 'synonym'] \\\n",
    "    .groupby('icd11_mms_en_id')['name'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65519a4a-f975-441c-9ba4-7a2e25fea1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e15f87da0a44290b2a6c6ffd90ed3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gerando JSON:   0%|          | 0/36782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gerado icd11_vector_input.json com 125885 conceitos.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 6. Montar o JSON final ---\n",
    "\n",
    "vector_input = []\n",
    "for _, row in tqdm(icd11_df.iterrows(), total=len(icd11_df), desc=\"Gerando JSON\"):\n",
    "    cid = row['id']\n",
    "    title = row['title']\n",
    "    code = row['code']\n",
    "    is_leaf = bool(row['is_leaf'])\n",
    "    entity_id_residual = row['entity_id_residual']\n",
    "    chapter_no = row.get('chapter_no')\n",
    "\n",
    "    # code_type\n",
    "    if not code:\n",
    "        code_type = \"foundation\"\n",
    "    elif str(code).startswith(\"X\"):\n",
    "        code_type = \"extension\"\n",
    "    else:\n",
    "        code_type = \"stem\"\n",
    "\n",
    "    # chapters\n",
    "    chapters = []\n",
    "    if pd.notnull(chapter_no):\n",
    "        for ch in str(chapter_no).replace(',', ';').replace(' ', ';').split(';'):\n",
    "            ch = ch.strip()\n",
    "            if ch in icd11_chapters:\n",
    "                chapters.append({\n",
    "                    \"code\": ch,\n",
    "                    \"title\": icd11_chapters[ch]\n",
    "                })\n",
    "\n",
    "    # blocks\n",
    "    blocks = get_blocks(row)\n",
    "\n",
    "    # parents\n",
    "    parents = get_parents(get_entity_id_base(entity_id_residual))\n",
    "\n",
    "    # leaf_related_codes\n",
    "    opts = postcoord_grouped.get_group(cid) if cid in postcoord_grouped.groups else pd.DataFrame()\n",
    "    leaf_related_codes = [opt_row['code'] for _, opt_row in opts.iterrows() if pd.notnull(opt_row['code'])]\n",
    "\n",
    "    # has_required_postcoordination\n",
    "    has_required_postcoordination = opts['required'].any() if not opts.empty and 'required' in opts else False\n",
    "\n",
    "    # --- 6a. Registro FSN principal ---\n",
    "    metadata_fsn = {\n",
    "        \"code\": code,\n",
    "        \"entity_id_residual\": entity_id_residual,\n",
    "        \"code_type\": code_type,\n",
    "        \"name_type\": \"fsn\",\n",
    "        \"is_leaf\": is_leaf,\n",
    "        \"chapter\": chapters,\n",
    "        \"blocks\": blocks,\n",
    "        \"parents\": parents,\n",
    "        \"has_required_postcoordination\": bool(has_required_postcoordination),\n",
    "        \"leaf_related_codes\": leaf_related_codes\n",
    "    }\n",
    "    vector_input.append({\n",
    "        \"concept_name\": title,\n",
    "        \"metadata\": metadata_fsn\n",
    "    })\n",
    "\n",
    "    # --- 6b. Registros de synonym (um para cada syn√¥nimo) ---\n",
    "    synonyms = synonyms_map.get(cid, [])\n",
    "    for syn in synonyms:\n",
    "        metadata_syn = metadata_fsn.copy()\n",
    "        metadata_syn[\"name_type\"] = \"synonym\"\n",
    "        vector_input.append({\n",
    "            \"concept_name\": syn,\n",
    "            \"metadata\": metadata_syn\n",
    "        })\n",
    "\n",
    "# --- 7. Salvar em JSON ---\n",
    "with open(\"icd11_vector_input.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vector_input, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Gerado icd11_vector_input.json com {len(vector_input)} conceitos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13e30a-84d3-4a5a-a6a8-af362ef1712c",
   "metadata": {},
   "source": [
    "### Testing Preliminar dataset (QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29febf26-411a-43e2-93f4-854c9735a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de conceitos is_leaf == False: 7244\n",
      "\n",
      "Distribui√ß√£o de code_type (n√≠vel raiz dos conceitos):\n",
      "foundation: 1443\n",
      "stem: 101352\n",
      "extension: 23090\n",
      "\n",
      "Total de c√≥digos marcados como 'stem' mas come√ßam com 'X': 0\n",
      "\n",
      "Exemplos:\n",
      "\n",
      "Top 10 conceitos com mais op√ß√µes de p√≥s-coordena√ß√£o:\n",
      "Intentional self-harm by water transport injury event with water vessel not damaged, disabled or destroyed causing submersion or drowning: 8152 op√ß√µes\n",
      "Intentional self-harm by water transport injury event with water vessel not damaged, disabled or destroyed causing other injury: 8152 op√ß√µes\n",
      "Assault by causing a fall or jump on same level or from less than 1 metre: 8014 op√ß√µes\n",
      "Assault by causing a fall on ice or snow: 8014 op√ß√µes\n",
      "Assault by causing a fall on other specified same level: 8014 op√ß√µes\n",
      "Assault by causing a fall on pedestrian conveyance: 8014 op√ß√µes\n",
      "Assault by causing a fall on unspecified same level: 8014 op√ß√µes\n",
      "Assault by causing a fall or jump from a height of 1 metre or more: 8014 op√ß√µes\n",
      "Assault by causing a fall or jump from 1 metre or more from building or structure: 8014 op√ß√µes\n",
      "Assault by causing a fall or jump from 1 metre or more from cliff: 8014 op√ß√µes\n",
      "\n",
      "Total de conceitos com has_required_postcoordination == True: 3069\n",
      "\n",
      "Total de sin√¥nimos: 89103\n",
      "\n",
      "Total de conceitos com mais de um parent: 124908\n",
      "Top 10 conceitos com mais parents:\n",
      "D1 - first diagonal branch: 14 parents\n",
      "D2 - second diagonal branch: 14 parents\n",
      "D3 - third diagonal branch: 14 parents\n",
      "Inferior gastric lymph node: 13 parents\n",
      "Upper superior gastric lymph node: 13 parents\n",
      "Superior gastric lymph node: 13 parents\n",
      "Lower superior gastric lymph node: 13 parents\n",
      "Paracardial superior gastric lymph node: 13 parents\n",
      "Pyloric lymph node: 13 parents\n",
      "Subpyloric lymph node: 13 parents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Carregar JSON\n",
    "with open(\"icd11_vector_input.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. Quantos conceitos is_leaf == False\n",
    "not_leaf = [d for d in data if not d['metadata'].get('is_leaf', True)]\n",
    "print(f\"Total de conceitos is_leaf == False: {len(not_leaf)}\")\n",
    "\n",
    "# 3. Contagem geral por tipo do conceito principal\n",
    "root_type_counts = Counter(d['metadata'].get('code_type', 'MISSING') for d in data)\n",
    "print(\"\\nDistribui√ß√£o de code_type (n√≠vel raiz dos conceitos):\")\n",
    "for k, v in root_type_counts.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# 4. Verificar inconsist√™ncia entre code_type e o prefixo do code\n",
    "mismatch = [\n",
    "    d for d in data\n",
    "    if d['metadata'].get('code_type') == 'stem' and str(d['metadata'].get('code', '')).startswith('X')\n",
    "]\n",
    "print(f\"\\nTotal de c√≥digos marcados como 'stem' mas come√ßam com 'X': {len(mismatch)}\")\n",
    "print(\"\\nExemplos:\")\n",
    "for item in mismatch[:10]:\n",
    "    print(f\"{item['metadata']['code']}: {item['concept_name']}\")\n",
    "\n",
    "# 5. Top 10 conceitos com mais op√ß√µes de p√≥s-coordena√ß√£o (agora via leaf_related_codes)\n",
    "postcoord_counts = [(d['concept_name'], len(d['metadata'].get('leaf_related_codes', [])))\n",
    "                    for d in data]\n",
    "top_10_postcoord = sorted(postcoord_counts, key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop 10 conceitos com mais op√ß√µes de p√≥s-coordena√ß√£o:\")\n",
    "for name, count in top_10_postcoord:\n",
    "    print(f\"{name}: {count} op√ß√µes\")\n",
    "\n",
    "# 6. Quantos conceitos t√™m has_required_postcoordination == True\n",
    "required_count = sum(1 for d in data if d['metadata'].get('has_required_postcoordination', False))\n",
    "print(f\"\\nTotal de conceitos com has_required_postcoordination == True: {required_count}\")\n",
    "\n",
    "# 7. Contar o total de sin√¥nimos (name_type == \"synonym\")\n",
    "synonym_count = sum(1 for d in data if d['metadata'].get('name_type') == 'synonym')\n",
    "print(f\"\\nTotal de sin√¥nimos: {synonym_count}\")\n",
    "\n",
    "# 8. Conceitos com mais de um parent\n",
    "multi_parent = [(d['concept_name'], len(d['metadata'].get('parents', [])))\n",
    "                for d in data if len(d['metadata'].get('parents', [])) > 1]\n",
    "multi_parent_sorted = sorted(multi_parent, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTotal de conceitos com mais de um parent: {len(multi_parent)}\")\n",
    "print(\"Top 10 conceitos com mais parents:\")\n",
    "for name, count in multi_parent_sorted[:10]:\n",
    "    print(f\"{name}: {count} parents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f6a656-dc80-4b55-9f6a-597a485e2d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'concept_name': 'None',\n",
      "  'metadata': {'blocks': [],\n",
      "               'chapter': [{'code': 'X', 'title': 'Extension Codes'}],\n",
      "               'code': 'XS8H',\n",
      "               'code_type': 'extension',\n",
      "               'entity_id_residual': '761934509',\n",
      "               'has_required_postcoordination': False,\n",
      "               'is_leaf': True,\n",
      "               'leaf_related_codes': [],\n",
      "               'name_type': 'fsn',\n",
      "               'parents': [{'code': '',\n",
      "                            'entity_id_residual': '1726150948',\n",
      "                            'title': 'Mild Moderate Severe Scale Value'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '146631584',\n",
      "                            'title': 'Generic Severity Scale Value'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '815889539',\n",
      "                            'title': 'Severity Scale Value'},\n",
      "                           {'code': 'X',\n",
      "                            'entity_id_residual': '979408586',\n",
      "                            'title': 'Extension Codes'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '455013390',\n",
      "                            'title': 'ICD Category'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '448895267',\n",
      "                            'title': 'ICD Entity'}]}},\n",
      " {'concept_name': 'Mild',\n",
      "  'metadata': {'blocks': [],\n",
      "               'chapter': [{'code': 'X', 'title': 'Extension Codes'}],\n",
      "               'code': 'XS5W',\n",
      "               'code_type': 'extension',\n",
      "               'entity_id_residual': '562478860',\n",
      "               'has_required_postcoordination': False,\n",
      "               'is_leaf': True,\n",
      "               'leaf_related_codes': [],\n",
      "               'name_type': 'fsn',\n",
      "               'parents': [{'code': '',\n",
      "                            'entity_id_residual': '1726150948',\n",
      "                            'title': 'Mild Moderate Severe Scale Value'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '146631584',\n",
      "                            'title': 'Generic Severity Scale Value'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '815889539',\n",
      "                            'title': 'Severity Scale Value'},\n",
      "                           {'code': 'X',\n",
      "                            'entity_id_residual': '979408586',\n",
      "                            'title': 'Extension Codes'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '455013390',\n",
      "                            'title': 'ICD Category'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '448895267',\n",
      "                            'title': 'ICD Entity'}]}},\n",
      " {'concept_name': 'Moderate',\n",
      "  'metadata': {'blocks': [],\n",
      "               'chapter': [{'code': 'X', 'title': 'Extension Codes'}],\n",
      "               'code': 'XS0T',\n",
      "               'code_type': 'extension',\n",
      "               'entity_id_residual': '1663264387',\n",
      "               'has_required_postcoordination': False,\n",
      "               'is_leaf': True,\n",
      "               'leaf_related_codes': [],\n",
      "               'name_type': 'fsn',\n",
      "               'parents': [{'code': '',\n",
      "                            'entity_id_residual': '1726150948',\n",
      "                            'title': 'Mild Moderate Severe Scale Value'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '146631584',\n",
      "                            'title': 'Generic Severity Scale Value'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '815889539',\n",
      "                            'title': 'Severity Scale Value'},\n",
      "                           {'code': 'X',\n",
      "                            'entity_id_residual': '979408586',\n",
      "                            'title': 'Extension Codes'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '455013390',\n",
      "                            'title': 'ICD Category'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '448895267',\n",
      "                            'title': 'ICD Entity'}]}}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "vector_input = json.load(open(\"icd11_vector_input.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Seleciona 3 itens com code_type == 'extension'\n",
    "ext_items = [item for item in vector_input if item['metadata'].get('code_type') == 'extension'][:3]\n",
    "pprint(ext_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c8bb60-c697-4777-9bb9-3e4fda220d09",
   "metadata": {},
   "source": [
    "### Creating Alpaca format dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc030f2-91fd-49ec-a110-eab07061e253",
   "metadata": {},
   "source": [
    "#### 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff10e97-354a-4852-bbcb-9cf341402c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting qdrant-client[fastembed]\n",
      "  Downloading qdrant_client-1.14.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fastembed==0.6.1 (from qdrant-client[fastembed])\n",
      "  Downloading fastembed-0.6.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting grpcio>=1.41.0 (from qdrant-client[fastembed])\n",
      "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting httpx>=0.20.0 (from httpx[http2]>=0.20.0->qdrant-client[fastembed])\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.11/site-packages (from qdrant-client[fastembed]) (1.24.4)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client[fastembed])\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /opt/conda/lib/python3.11/site-packages (from qdrant-client[fastembed]) (4.24.3)\n",
      "Collecting pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 (from qdrant-client[fastembed])\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.26.14 in /opt/conda/lib/python3.11/site-packages (from qdrant-client[fastembed]) (2.0.7)\n",
      "Collecting huggingface-hub<1.0,>=0.20 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting loguru<0.8.0,>=0.7.2 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting mmh3<6.0.0,>=4.1.0 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnxruntime!=1.20.0,>=1.17.0 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting pillow<12.0.0,>=10.3.0 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting py-rust-stemmers<0.2.0,>=0.1.0 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading py_rust_stemmers-0.1.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in /opt/conda/lib/python3.11/site-packages (from fastembed==0.6.1->qdrant-client[fastembed]) (2.31.0)\n",
      "Collecting tokenizers<1.0,>=0.15 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in /opt/conda/lib/python3.11/site-packages (from fastembed==0.6.1->qdrant-client[fastembed]) (4.66.1)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch)\n",
      "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.3.0->torch) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed])\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (3.4)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed])\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client[fastembed])\n",
      "  Downloading h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.20->fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed])\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed])\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed])\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.31->fastembed==0.6.1->qdrant-client[fastembed]) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed])\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed])\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting coloredlogs (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (1.3.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed==0.6.1->qdrant-client[fastembed])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Downloading fastembed-0.6.1-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m512.1/512.1 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m444.2/444.2 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading qdrant_client-1.14.2-py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m327.7/327.7 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading py_rust_stemmers-0.1.5-cp311-cp311-manylinux_2_28_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py-rust-stemmers, nvidia-cusparselt-cu12, flatbuffers, typing-extensions, triton, sympy, safetensors, regex, portalocker, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mmh3, loguru, hyperframe, humanfriendly, hpack, hf-xet, h11, grpcio, filelock, annotated-types, typing-inspection, pydantic-core, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, httpcore, h2, coloredlogs, tokenizers, pydantic, onnxruntime, nvidia-cusolver-cu12, httpx, transformers, torch, fastembed, torchvision, sentence-transformers, qdrant-client\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 10.1.0\n",
      "    Uninstalling Pillow-10.1.0:\n",
      "      Successfully uninstalled Pillow-10.1.0\n",
      "Successfully installed annotated-types-0.7.0 coloredlogs-15.0.1 fastembed-0.6.1 filelock-3.18.0 flatbuffers-25.2.10 grpcio-1.71.0 h11-0.16.0 h2-4.2.0 hf-xet-1.1.2 hpack-4.1.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.32.3 humanfriendly-10.0 hyperframe-6.1.0 loguru-0.7.3 mmh3-5.1.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 onnxruntime-1.22.0 pillow-11.2.1 portalocker-2.10.1 py-rust-stemmers-0.1.5 pydantic-2.11.5 pydantic-core-2.33.2 qdrant-client-1.14.2 regex-2024.11.6 safetensors-0.5.3 sentence-transformers-4.1.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 torchvision-0.22.0 transformers-4.52.4 triton-3.3.0 typing-extensions-4.13.2 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client[fastembed] torch torchvision transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca14189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b85a5cc90a46fe93d3022be6b008e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9eb019a3444b93885b0f4e98ad4c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64290861c0824e5b9739d243e5442434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c295a019c6724c4cb262e42c824dc10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c241dd9ac5ae45bfbce845b1a0424b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe9c8e7ca0e4f19acc1f343f70ed86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be574e102c874a63ac02ddb6a4ec303f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec30215972bc4102ba4b80dad41cd855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd52580715f4d70bebfac8ebb64fddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9072311153854ba1ab77ee6640c7d575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db73860f0d04fa5bb7023953ed65b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "col_name_map = {\n",
    "    \"icd11_concepts_minilm\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"icd11_concepts_mpnet\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"icd11_concepts_biobert\": \"pritamdeka/S-BioBert-snli-multinli-stsb\"\n",
    "}\n",
    "\n",
    "# --- CONFIGS ---\n",
    "QDRANT_HOST = \"qdrant.filipelopes.me\"\n",
    "QDRANT_PORT = 80\n",
    "COLLECTION_NAME = \"icd11_concepts_mpnet\"\n",
    "EMBEDDING_MODEL = col_name_map[COLLECTION_NAME]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer(EMBEDDING_MODEL, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19402c42-663a-4c3b-9adb-207b68090c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset v2 ‚Äì ICD-11 mapping in Alpaca format\n",
    "===========================================\n",
    "\n",
    "‚Ä¢ Obt√©m conceitos (FSN + sin√¥nimos) do JSON vetorial.\n",
    "‚Ä¢ Busca candidatos no Qdrant (top-7) + dados relacionais.\n",
    "‚Ä¢ Gera contexto, linha de racioc√≠nio (<scratchpad>) e sa√≠da final.\n",
    "‚Ä¢ Produz JSONL pronto para fine-tuning.\n",
    "\n",
    "Pr√©-requisitos:\n",
    "---------------\n",
    "pip install qdrant-client tqdm sqlalchemy pymysql python-dotenv\n",
    "Vari√°veis de ambiente: DB_*, QDRANT_HOST, QDRANT_PORT\n",
    "\"\"\"\n",
    "\n",
    "import os, json, random, copy\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import SearchRequest, Filter\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 1. CONEX√ïES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# sqlachemy engine was already create on previous blocks\n",
    "\n",
    "QDRANT_HOST=\"qdrant.filipelopes.me\"\n",
    "QDRANT_PORT=80\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=QDRANT_HOST,\n",
    "    port=QDRANT_PORT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132feba-c220-4b2f-95dd-8080a17fe2e9",
   "metadata": {},
   "source": [
    "#### 2. Helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7ad1eb1d-9765-41e8-b877-9ef4ed8239fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2. FUN√á√ïES DE APOIO ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n",
    "\n",
    "def dedup_code_results(results, prefer_name_type=\"fsn\"):\n",
    "    \"\"\"\n",
    "    Remove duplicados de c√≥digo, mantendo apenas um por code.\n",
    "    Prioriza registros com name_type == prefer_name_type.\n",
    "    \"\"\"\n",
    "    filtered = {}\n",
    "    for r in results:\n",
    "        code = (r.payload.get('code') or '').strip()\n",
    "        name_type = r.payload.get('name_type')\n",
    "        # Se nunca adicionado ou o prefer_name_type est√° presente e o j√° salvo n√£o √© preferido\n",
    "        if code not in filtered:\n",
    "            filtered[code] = r\n",
    "        else:\n",
    "            already = filtered[code].payload.get('name_type')\n",
    "            # Se prefer_name_type √© encontrado e j√° salvo n√£o √© preferido, substitui\n",
    "            if name_type == prefer_name_type and already != prefer_name_type:\n",
    "                filtered[code] = r\n",
    "            # Se ambos t√™m o mesmo prefer_name_type, mant√©m o primeiro (poderia escolher pelo maior score)\n",
    "    \n",
    "    return list(filtered.values())\n",
    "\n",
    "def search_candidates(text: str, top_k: int = 7):\n",
    "    \"\"\"\n",
    "    Consulta Qdrant e devolve lista [(payload, distance), ‚Ä¶], sem c√≥digos duplicados (prioriza fsn).\n",
    "    \"\"\"\n",
    "    res = qdrant.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=get_embedding(text),\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        query_filter=Filter(\n",
    "            must=[\n",
    "                {\"key\": \"code_type\", \"match\": {\"value\": \"stem\"}}\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    # Remove duplicados de code antes de retornar, priorizando fsn\n",
    "    deduped = dedup_code_results(res.points)\n",
    "    # S√≥ retorna caso o score seja maior que 0.8\n",
    "    return [(r.payload, r.score) for r in deduped if r.score > 0.8]\n",
    "\n",
    "\n",
    "def needs_postcoord(code:str) -> bool:\n",
    "    # retorna true caso concept_name contenha & ou /\n",
    "    return \"&\" in code or \"/\" in code\n",
    "\n",
    "\n",
    "def fetch_postcoord_options(concept: str, candidates: list[dict]) -> list[dict]:\n",
    "    \"\"\"Retorna op√ß√µes de p√≥s-coordena√ß√£o relevantes a partir do qdrant, filtrando pelos c√≥digos dos candidatos e lendo o metadado leaf_related_code\"\"\"\n",
    "    # Extrai todos os c√≥digos de leaf_related_codes (que √© uma lista de strings por candidato)\n",
    "    # Extrai e agrega todos os c√≥digos de leaf_related_codes dos candidatos\n",
    "    related_codes = set(code for c, _ in candidates for code in c.get(\"leaf_related_codes\", []))\n",
    "\n",
    "    res = qdrant.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=get_embedding(concept),\n",
    "        limit=10,\n",
    "        with_payload=True,\n",
    "        query_filter=Filter(\n",
    "            must=[\n",
    "                {\"key\": \"code\", \"match\": {\"any\": related_codes}}\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    # retona o title e code dos top 10\n",
    "    return [{\"code\": r.payload[\"code\"], \"title\": r.payload[\"concept_name\"]} for r in res.points[:10]]\n",
    "\n",
    "def get_title_by_code(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Consulta r√°pida ao Qdrant para obter 'concept_name' pelo c√≥digo exato.\n",
    "    Se n√£o encontrar, devolve o pr√≥prio c√≥digo.\n",
    "    \"\"\"\n",
    "    res = qdrant.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        with_payload=True,\n",
    "        limit=1,\n",
    "        query_filter=Filter(must=[{\"key\": \"code\", \"match\": {\"value\": code}}, {\"key\": \"name_type\", \"match\": {\"value\": \"fsn\"}}]),\n",
    "    )\n",
    "    if res.points:\n",
    "        return res.points[0].payload.get(\"concept_name\", code)\n",
    "    return code\n",
    "    \n",
    "def build_input_context(final_title: str, final_code: str, top_k: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Cria o contexto do prompt:\n",
    "      ‚Ä¢ t√≠tulo do conceito\n",
    "      ‚Ä¢ stem-codes candidatos (deduplicados)\n",
    "      ‚Ä¢ op√ß√µes de p√≥s-coordena√ß√£o (deduplicadas) contendo\n",
    "        *obrigatoriamente* todos os c√≥digos presentes em `final_code`.\n",
    "    \"\"\"\n",
    "\n",
    "    # ‚îÄ‚îÄ 1. Stem-codes candidatos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    lines = [f\"Clinical concept to map: <input>{final_title}</input>\"]\n",
    "    candidates = search_candidates(final_title)\n",
    "    codes = [payload.get('code') for payload, score in candidates]\n",
    "    # caso tenha correspondente com score > 0.8 (j√° verificado em search_candidates) e tamb√©m caso algum dos codes estejam contidos em final_code\n",
    "    if candidates and any(c[0]['code'] in final_code for c in candidates) and len(candidates) >= 2:\n",
    "        lines.append(\"Relevant matched stem codes found:\")\n",
    "        for payload, score in candidates:\n",
    "            lines.append(f\"- {payload['code']} - {payload['concept_name']}\")\n",
    "    else:\n",
    "        return \"\\n\".join(lines)  # retorna apenas o t√≠tulo se n√£o houver candidatos\n",
    "\n",
    "    if needs_postcoord(final_code):\n",
    "        # ‚îÄ‚îÄ 2. P√≥s-coordena√ß√£o sugerida ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        post_opts = fetch_postcoord_options(final_title, candidates)\n",
    "    \n",
    "        # a) deduplicar lista retornada\n",
    "        seen_ext = set()\n",
    "        dedup_post_opts = []\n",
    "        for o in post_opts:\n",
    "            c = o[\"code\"].strip()\n",
    "            if c not in seen_ext:\n",
    "                seen_ext.add(c)\n",
    "                dedup_post_opts.append(o)\n",
    "    \n",
    "        # b) garantir que TODO c√≥digo presente em final_code conste na lista\n",
    "        parts_in_final = re.split(r'[&/]', final_code)\n",
    "        for part in parts_in_final[1:]:  # ignora o stem code principal\n",
    "            part = part.strip()\n",
    "            if part and part not in seen_ext:\n",
    "                dedup_post_opts.append(\n",
    "                    {\"code\": part, \"title\": get_title_by_code(part)}\n",
    "                )\n",
    "                seen_ext.add(part)\n",
    "    \n",
    "        # c) adicionar ao contexto (m√°x. 10 para n√£o estourar prompt)\n",
    "        if dedup_post_opts:\n",
    "            lines.append(\"Post-coordination options:\")\n",
    "            for o in dedup_post_opts[:10]:\n",
    "                lines.append(f\"- {o['code']} ({o['title']})\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "\n",
    "def build_scratchpad(\n",
    "    code: str,\n",
    "    map_type: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Gera bloco <scratchpad> em ingl√™s com racioc√≠nio passo-a-passo para o mapeamento de um conceito ICD-11.\n",
    "    Explica a escolha dos stem codes, extens√µes e clusters utilizados.\n",
    "    \"\"\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    # Caso seja uma extension code n√£o retorna nada\n",
    "    if code.startswith('X') and '&' not in code and '/' not in code:\n",
    "        return \"<scratchpad>Extension codes do not require a scratchpad explanation.</scratchpad>\"\n",
    "\n",
    "    # 1. Divide o c√≥digo em partes (stem codes e extensions), mantendo a ordem\n",
    "    code_parts = re.split(r'[/&]', code)\n",
    "\n",
    "    # 2. Busca no Qdrant apenas pelo filtro de c√≥digo (n√£o √© uma busca sem√¢ntica)\n",
    "    res = qdrant.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        with_payload=True,\n",
    "        limit=len(code_parts) + 5,\n",
    "        query_filter=Filter(\n",
    "            must=[\n",
    "                {\"key\": \"code\", \"match\": {\"any\": code_parts}},\n",
    "                {\"key\": \"name_type\", \"match\": {\"value\": \"fsn\"}}\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 3. Inicializa√ß√£o do scratchpad (explica√ß√£o em ingl√™s)\n",
    "    pad = []\n",
    "    pad.append(\n",
    "        \"We are verifying the stem code that is most related to the concept. \"\n",
    "        \"Ideally, it should not be more specific (i.e., contain elements not present in the original concept to be mapped), \"\n",
    "        \"nor less specific (i.e., miss elements present in the concept to be mapped but not represented in the suggested stem code).\"\n",
    "    )\n",
    "\n",
    "    # 4. Ordem dos stem codes (usado apenas para rotular na explica√ß√£o)\n",
    "    stem_code_order = [\"primary\", \"secondary\", \"tertiary\", \"quaternary\"]\n",
    "\n",
    "    # 5. Flag para explicar p√≥s-coordena√ß√£o s√≥ uma vez\n",
    "    needed_postcoordination = \"&\" in code or \"/\" in code\n",
    "\n",
    "    # 6. Processa cada parte do c√≥digo apenas UMA VEZ\n",
    "    for idx, part in enumerate(code_parts):\n",
    "        found = False  # Garante que s√≥ processa o primeiro match para cada part\n",
    "        for r in res.points:\n",
    "            if r.payload.get(\"code\") == part:\n",
    "                found = True\n",
    "                chapter = r.payload.get('chapter', [])\n",
    "                title = r.payload.get('concept_name', '')\n",
    "                if idx == 0:  # Primeira parte = stem code principal\n",
    "                    stem_code = part\n",
    "                    # Explica cap√≠tulo e blocos\n",
    "                    if chapter:\n",
    "                        pad.append(\n",
    "                            f\"The closest chapter is <chapter>{chapter[0]['code']} - {chapter[0]['title']}</chapter>.\"\n",
    "                        )\n",
    "                    for idxb, block in enumerate(r.payload.get(\"blocks\", [])):\n",
    "                        code_part = block['code'].split('-')[-1] if '-' in block['code'] else block['code']\n",
    "                        if idxb == 0:\n",
    "                            pad.append(f\"The nearest block is <blockL{idxb+1}>{code_part} - {block['title']}</blockL{idxb+1}>.\")\n",
    "                        else:\n",
    "                            pad.append(f\"I found another block within this that is related to the topic: <blockL{idxb+1}>{code_part} - {block['title']}</blockL{idxb+1}>.\")\n",
    "                    if stem_code_order:\n",
    "                        pad.append(f\"Selected {stem_code_order[0]} stem code: {stem_code} - {get_title_by_code(stem_code)}.\")\n",
    "                        stem_code_order = stem_code_order[1:]  # Remove o usado\n",
    "                    else:\n",
    "                        pad.append(f\"Selected stem code: {stem_code} - {get_title_by_code(stem_code)}.\")\n",
    "                else:\n",
    "                    # S√≥ explica p√≥s-coordena√ß√£o uma vez\n",
    "                    if needed_postcoordination:\n",
    "                        has_required = r.payload.get(\"has_required_postcoordination\", False)\n",
    "                        pad.append(\n",
    "                            \"Evaluating whether the term requires post-coordination: according to ICD-11, this concept \"\n",
    "                            + (\"REQUIRES\" if has_required else \"does NOT require\")\n",
    "                            + \" post-coordination.\"\n",
    "                        )\n",
    "                        pad.append(\n",
    "                            \"Semantically, we check if the stem code found fully covers the concept to be mapped. \"\n",
    "                            \"If not, we search for stem codes or extension codes that complement the specificity of the concept.\"\n",
    "                        )\n",
    "                        needed_postcoordination = False  # Explica s√≥ uma vez\n",
    "                    # Descreve extens√µes ou stems adicionais\n",
    "                    if r.payload.get(\"code_type\") == \"extension\":\n",
    "                        # Explica a dimens√£o trazida pela extens√£o, se dispon√≠vel\n",
    "                        parent_text = \"\"\n",
    "                        parents = r.payload.get('parents', [])\n",
    "                        parent_title = \"\"\n",
    "                        if len(parents) >= 5:\n",
    "                            parent_title = parents[-5]['title']\n",
    "                        elif len(parents) >= 4:\n",
    "                            parent_title = parents[-4]['title']\n",
    "                        if parent_title:\n",
    "                            parent_text = f\"in order to include the dimension of {parent_title} to its meaning.\"\n",
    "                        pad.append(\n",
    "                            f\"After analysis, it is necessary to add the extension \\\"{part} - {title}\\\" to the main stem code {parent_text}\"\n",
    "                        )\n",
    "                    elif r.payload.get(\"code_type\") == \"stem\":\n",
    "                        if stem_code_order:\n",
    "                            pad.append(f\"After review, I believe a {stem_code_order[0]} stem code \\\"{part} - {title}\\\" is necessary to complement the meaning and specificity of the concept.\")\n",
    "                            stem_code_order = stem_code_order[1:]  # Remove usado\n",
    "                        else:\n",
    "                            pad.append(f\"After review, I believe the stem code {part} is necessary to complement the concept.\")\n",
    "                break  # <-- Sai do loop interno ap√≥s primeiro match!\n",
    "        if not found:\n",
    "            # Opcional: log ou debug, se algum part n√£o foi encontrado\n",
    "            pass\n",
    "\n",
    "    # 7. Finaliza√ß√£o: mensagem se p√≥s-coordena√ß√£o foi necess√°ria ou n√£o\n",
    "    if len(code_parts) == 1 and not needs_postcoord(code_parts[0]):\n",
    "        pad.append(\n",
    "            \"After analysis, the following stem code is sufficient to represent the concept.\"\n",
    "        )\n",
    "    else:\n",
    "        pad.append(\n",
    "            \"By combining secondary codes with a slash ‚Äú/‚Äù and extensions with ‚Äú&‚Äù, I need to assess whether, after post-coordination, we have a final concept that is fully equivalent to the input concept.\"\n",
    "        )\n",
    "\n",
    "    # 8. Declara o tipo de mapeamento\n",
    "    if map_type == \"SAME-AS\":\n",
    "        pad.append(\n",
    "            \"The final concept covers all the specificity of the concept to be mapped, therefore here is the final code with the mapping type:\"\n",
    "        )\n",
    "    elif map_type == \"NARROWER-THAN\":\n",
    "        pad.append(\"The input concept is more specific than the final coded concept, therefore here is the final code with the mapping type:\")\n",
    "    elif map_type == \"BROADER-THAN\":\n",
    "        pad.append(\"The input concept is less specific than the final coded concept, therefore here is the final code with the mapping type:\")\n",
    "\n",
    "    # 9. Retorna bloco formatado\n",
    "    return \"<scratchpad>\\n\" + \"\\n\".join(pad) + \"\\n</scratchpad>\"\n",
    "\n",
    "\n",
    "def compute_weight(code, name_type, category):\n",
    "    \"\"\"Calcula o peso baseado no tipo de nome, origem e se h√° p√≥s-coordena√ß√£o no c√≥digo.\"\"\"\n",
    "    base = 1.0\n",
    "\n",
    "    # Penaliza sin√¥nimo ou par√°frase\n",
    "    if name_type == \"synonym\":\n",
    "        base *= 0.7\n",
    "    elif name_type == \"paraphrase\":\n",
    "        base *= 0.5\n",
    "\n",
    "    # Aumenta peso se for do CIEL\n",
    "    if \"ciel\" in category.lower():\n",
    "        base *= 1.2\n",
    "\n",
    "    # Aumenta peso se tiver p√≥s-coordena√ß√£o\n",
    "    if \"&\" in code or \"/\" in code:\n",
    "        base *= 1.5\n",
    "\n",
    "    return round(base, 2)\n",
    "\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    \"\"\"Gera embedding usando o modelo sentence-transformers/all-mpnet-base-v2.\"\"\"\n",
    "    embedding = model.encode(text, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "481a5699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'concept_name': 'Certain infectious or parasitic diseases',\n",
      "  'metadata': {'blocks': [],\n",
      "               'chapter': [{'code': '01',\n",
      "                            'title': 'Certain infectious or parasitic '\n",
      "                                     'diseases'}],\n",
      "               'code': None,\n",
      "               'code_type': 'foundation',\n",
      "               'entity_id_residual': '1435254666',\n",
      "               'has_required_postcoordination': False,\n",
      "               'is_leaf': False,\n",
      "               'leaf_related_codes': [],\n",
      "               'name_type': 'fsn',\n",
      "               'parents': [{'code': '',\n",
      "                            'entity_id_residual': '455013390',\n",
      "                            'title': 'ICD Category'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '448895267',\n",
      "                            'title': 'ICD Entity'}]}},\n",
      " {'concept_name': 'Gastroenteritis or colitis of infectious origin',\n",
      "  'metadata': {'blocks': [],\n",
      "               'chapter': [{'code': '01',\n",
      "                            'title': 'Certain infectious or parasitic '\n",
      "                                     'diseases'}],\n",
      "               'code': None,\n",
      "               'code_type': 'foundation',\n",
      "               'entity_id_residual': '588616678',\n",
      "               'has_required_postcoordination': False,\n",
      "               'is_leaf': False,\n",
      "               'leaf_related_codes': [],\n",
      "               'name_type': 'fsn',\n",
      "               'parents': [{'code': '01',\n",
      "                            'entity_id_residual': '1435254666',\n",
      "                            'title': 'Certain infectious or parasitic '\n",
      "                                     'diseases'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '455013390',\n",
      "                            'title': 'ICD Category'},\n",
      "                           {'code': '',\n",
      "                            'entity_id_residual': '448895267',\n",
      "                            'title': 'ICD Entity'}]}}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# amostra de conte√∫do do icd11_vector_input.json\n",
    "def load_vector_input(file_path: str):\n",
    "    \"\"\"Carrega o JSON vetorial de entrada.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "pprint(load_vector_input(\"icd11_vector_input.json\")[:2])  # Exemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949496d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ 2. Definir instru√ß√µes poss√≠veis\n",
    "instructions_stem = [\n",
    "    \"Provide the ICD-11 code for the clinical concept. Include extensions or secondary stem codes if required for full specificity.\",\n",
    "    \"Find the ICD-11 code corresponding to the clinical concept. Use extensions or secondary stem codes if necessary.\",\n",
    "    \"Map the clinical concept to its ICD-11 code. Add extensions or secondary stem codes if needed.\",\n",
    "    \"Assign the ICD-11 code to the following concept, considering extensions or secondary stem codes where appropriate.\"\n",
    "]\n",
    "\n",
    "instructions_extension = [\n",
    "    \"Provide the ICD-11 extension code corresponding to the specified extension concept.\",\n",
    "    \"Identify the ICD-11 extension code for the following extension concept.\",\n",
    "    \"Find the corresponding ICD-11 extension code for this modifier.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266171d-3f3d-41a1-8379-789c3de9560f",
   "metadata": {},
   "source": [
    "#### 3. Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3249e12b-ad64-47c3-a852-08135413c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando mapeamentos ICD-11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30966/30966 [51:21<00:00, 10.05it/s]  \n",
      "Processando mapeamentos CIEL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24366/24366 [43:23<00:00,  9.36it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset v2 salvo em ../assets/icd11_alpaca_v2.jsonl com 54789 exemplos.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 3. LOOP PRINCIPAL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n",
    "\n",
    "# captura os dados do banco de dados dos mapeamentos icd que j√° temos pela tabela icd11_mms_en\n",
    "query_icd_source_mappings = text(\"\"\"\n",
    "    SELECT code, title, is_leaf, class_kind, chapter_no FROM analytics.icd11_mms_en\n",
    "\"\"\")\n",
    "df_icd_source_mappings = pd.read_sql(query_icd_source_mappings, engine)\n",
    "df_icd_source_mappings = df_icd_source_mappings[\n",
    "    (df_icd_source_mappings['is_leaf']) &\n",
    "    (df_icd_source_mappings['code'].notnull()) &\n",
    "    (df_icd_source_mappings['class_kind'] == 'category')\n",
    "].sample(frac=1).reset_index(drop=True)  # embaralha\n",
    "\n",
    "# captura no banco de dados os mapeamentos no ciel que j√° temos\n",
    "query_ciel_source_mappings = text(\"\"\"\n",
    "    SELECT concept_id, fsn, map_type, icd11_code\n",
    "    FROM analytics.vw_ciel_diagnosis_n_findings_to_icd11\n",
    "\"\"\")\n",
    "df_ciel_source_mappings = pd.read_sql(query_ciel_source_mappings, engine)\n",
    "df_ciel_source_mappings = df_ciel_source_mappings.sample(frac=1).reset_index(drop=True)  # embaralha\n",
    "\n",
    "N_SAMPLES = 20  # ou outro valor\n",
    "\n",
    "# Amostra de cada fonte\n",
    "df_icd_sample = df_icd_source_mappings.sample(n=N_SAMPLES, random_state=42)\n",
    "df_ciel_sample = df_ciel_source_mappings.sample(n=N_SAMPLES, random_state=42)\n",
    "\n",
    "alpaca_entries = []\n",
    "\n",
    "# Etapa 1: Processar mapeamentos icd11_mms_en\n",
    "for _, row in tqdm(df_icd_source_mappings.iterrows(), total=len(df_icd_source_mappings), desc=\"Processando mapeamentos ICD-11\"):\n",
    "    code = row['code']\n",
    "    title = str(row['title']).lstrip('-‚Äì‚Äî ').strip()\n",
    "    chapter = row['chapter_no']\n",
    "\n",
    "    # Verifica se o c√≥digo √© um c√≥digo de extens√£o ou stem\n",
    "    if code.startswith('X') and chapter == 'X':\n",
    "        category = 'source:icdapi>extension'\n",
    "        instruction = random.choice(instructions_extension)\n",
    "    else:\n",
    "        category = 'source:icdapi>same-as+stem'\n",
    "        instruction = random.choice(instructions_stem)\n",
    "\n",
    "    entry = {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": build_input_context(final_title=title, final_code=code),\n",
    "        \"output\": f\"\"\"\n",
    "        {build_scratchpad(\n",
    "            code=code, map_type=\"SAME-AS\"\n",
    "        )}\n",
    "        <map_type>SAME-AS</map_type> <code>{code}</code> <category>{category}</category>\"\"\",\n",
    "        \"category\": category,\n",
    "        \"weight\": compute_weight(code=code, name_type=\"fsn\", category=category)\n",
    "    }\n",
    "\n",
    "    alpaca_entries.append(entry)\n",
    "\n",
    "# Etapa 2: Processar mapeamentos CIEL\n",
    "for _, row in tqdm(df_ciel_source_mappings.iterrows(), total=len(df_ciel_source_mappings), desc=\"Processando mapeamentos CIEL\"):\n",
    "    concept_id = row['concept_id']\n",
    "    fsn = str(row['fsn']).lstrip('-‚Äì‚Äî ').strip()\n",
    "    map_type = row['map_type']\n",
    "    icd11_code = row['icd11_code']\n",
    "\n",
    "    category = None\n",
    "    instruction = random.choice(instructions_stem)\n",
    "\n",
    "    if icd11_code.startswith('X'):\n",
    "        category = 'source:ciel>extension'\n",
    "        instruction = random.choice(instructions_extension)\n",
    "    if '&' not in icd11_code and '/' not in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+stem'\n",
    "        elif map_type == \"NARROWER-THAN\":\n",
    "            category = 'source:ciel>narrower-than+stem'\n",
    "        elif map_type == \"BROADER-THAN\":\n",
    "            category = 'source:ciel>broader-than+stem'\n",
    "\n",
    "    elif icd11_code.count('&') == 1 and '/' not in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+stem+extension'\n",
    "        elif map_type == \"NARROWER-THAN\":\n",
    "            category = 'source:ciel>narrower-than+stem+extension'\n",
    "        elif map_type == \"BROADER-THAN\":\n",
    "            category = 'source:ciel>broader-than+stem+extension'\n",
    "\n",
    "    elif icd11_code.count('&') > 1 and '/' not in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+stem+extensions'\n",
    "        elif map_type == \"NARROWER-THAN\":\n",
    "            category = 'source:ciel>narrower-than+stem+extensions'\n",
    "        elif map_type == \"BROADER-THAN\":\n",
    "            category = 'source:ciel>broader-than+stem+extensions'\n",
    "\n",
    "    elif '&' not in icd11_code and '/' in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+cluster'\n",
    "\n",
    "    elif '&' in icd11_code and '/' in icd11_code:\n",
    "        if map_type == \"SAME-AS\":\n",
    "            category = 'source:ciel>same-as+cluster+extension'\n",
    "\n",
    "    if category is None:\n",
    "        continue\n",
    "\n",
    "    entry = {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": build_input_context(final_title=fsn, final_code=icd11_code),\n",
    "        \"output\": f\"\"\"{build_scratchpad(\n",
    "            code=icd11_code, map_type=map_type\n",
    "        )}\\n<map_type>{map_type}</map_type> <code>{icd11_code}</code> <category>{category}</category>\"\"\",\n",
    "        \"category\": category,\n",
    "        \"weight\": compute_weight(code=icd11_code, name_type=\"fsn\", category=category)\n",
    "    }\n",
    "\n",
    "    alpaca_entries.append(entry)\n",
    "\n",
    "# ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 4. EXPORTAR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n",
    "out_path = \"../assets/icd11_alpaca_v2.jsonl\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for e in alpaca_entries:\n",
    "        json.dump(e, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Dataset v2 salvo em {out_path} com {len(alpaca_entries)} exemplos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1aa81-bf93-4381-b772-57a9c6eef50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "jsonl_path = \"../assets/icd11_alpaca_v2.jsonl\"\n",
    "\n",
    "# Carrega o dataset completo\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total de exemplos: {len(data)}\")\n",
    "\n",
    "# Sorteia 5 amostras aleat√≥rias sem repeti√ß√£o\n",
    "sample_n = 40\n",
    "random.seed(36)  # Reprodutibilidade (opcional)\n",
    "sampled = random.sample(data, min(sample_n, len(data)))\n",
    "\n",
    "for i, item in enumerate(sampled):\n",
    "    print(f\"\\n--- Exemplo {i+1} ---\")\n",
    "    print(f\"Category: {item.get('category')}\")\n",
    "    print(f\"Instruction: {item.get('instruction')}\")\n",
    "    print(f\"Input: {item.get('input')}\")\n",
    "    print(f\"Output: {item.get('output')}\")\n",
    "    print(f\"Weight: {item.get('weight')}\")\n",
    "    print(\"\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\")\n",
    "\n",
    "# Distribui√ß√£o por categoria\n",
    "cat_counts = Counter(item['category'] for item in data)\n",
    "print(\"\\nDistribui√ß√£o por categoria:\")\n",
    "for cat, count in cat_counts.items():\n",
    "    print(f\"{cat}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b946815d-9bc2-44a1-ae45-3aa0bd2c2d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados agrupados por code para: Benign vascular neoplasms of infancy or childhood, unspecified\n",
      "\n",
      "Code: 2E81.2Z (total: 2)\n",
      "   01. fsn: Benign vascular neoplasms of infancy or childhood, unspecified\n",
      "   02. synonym: Benign vascular neoplasms of infancy or childhood\n",
      "----------------------------------------\n",
      "Code: 2E81.2 (total: 1)\n",
      "   01. fsn: Benign vascular neoplasms of infancy or childhood\n",
      "----------------------------------------\n",
      "Code: 2E81.2Y (total: 1)\n",
      "   01. fsn: Other specified benign vascular neoplasms of infancy or childhood\n",
      "----------------------------------------\n",
      "Code: 2E81.Z (total: 2)\n",
      "   01. fsn: Benign vascular neoplasms, unspecified\n",
      "   02. synonym: Benign vascular neoplasms\n",
      "----------------------------------------\n",
      "Code: 2E81 (total: 1)\n",
      "   01. fsn: Benign vascular neoplasms\n",
      "----------------------------------------\n",
      "Code: 2E81.Y (total: 1)\n",
      "   01. fsn: Other specified benign vascular neoplasms\n",
      "----------------------------------------\n",
      "Code: 2F2Y (total: 1)\n",
      "   01. synonym: Benign proliferations or neoplasms of cutaneous blood vessels\n",
      "----------------------------------------\n",
      "Code: 2F3Z (total: 1)\n",
      "   01. fsn: Benign neoplasms except of mesenchymal origin, of unspecified site\n",
      "----------------------------------------\n",
      "Code: 2E90.6 (total: 1)\n",
      "   01. synonym: juvenile angiofibroma unspecified site\n",
      "----------------------------------------\n",
      "Code: 2F9Y (total: 1)\n",
      "   01. synonym: Congenital cardiac tumor, not otherwise specified\n",
      "----------------------------------------\n",
      "Code: 2F01 (total: 2)\n",
      "   01. fsn: Benign neoplasm of other intrathoracic organs\n",
      "   02. synonym: Heart tumour of the child\n",
      "----------------------------------------\n",
      "Code: 2F3Y (total: 2)\n",
      "   01. synonym: Benign neoplasm of lymph vessels\n",
      "   02. fsn: Benign neoplasms except of mesenchymal origin, of other specified site\n",
      "----------------------------------------\n",
      "Code: LC52 (total: 1)\n",
      "   01. fsn: Complex or combined developmental vascular malformations involving the skin\n",
      "----------------------------------------\n",
      "Code: 2E81.0Z (total: 1)\n",
      "   01. synonym: Benign angioendothelioma of unspecified site\n",
      "----------------------------------------\n",
      "Code: 2B5K (total: 1)\n",
      "   01. synonym: Malignant angioma of unspecified site\n",
      "----------------------------------------\n",
      "Code: 2F7Z (total: 1)\n",
      "   01. synonym: Embryonal tumour of unspecified site\n",
      "----------------------------------------\n",
      "\n",
      "Resumo por code:\n",
      "- 2E81.2Z: 2 resultados (fsn, synonym)\n",
      "- 2E81.2: 1 resultados (fsn)\n",
      "- 2E81.2Y: 1 resultados (fsn)\n",
      "- 2E81.Z: 2 resultados (fsn, synonym)\n",
      "- 2E81: 1 resultados (fsn)\n",
      "- 2E81.Y: 1 resultados (fsn)\n",
      "- 2F2Y: 1 resultados (synonym)\n",
      "- 2F3Z: 1 resultados (fsn)\n",
      "- 2E90.6: 1 resultados (synonym)\n",
      "- 2F9Y: 1 resultados (synonym)\n",
      "- 2F01: 2 resultados (fsn, synonym)\n",
      "- 2F3Y: 2 resultados (fsn, synonym)\n",
      "- LC52: 1 resultados (fsn)\n",
      "- 2E81.0Z: 1 resultados (synonym)\n",
      "- 2B5K: 1 resultados (synonym)\n",
      "- 2F7Z: 1 resultados (synonym)\n"
     ]
    }
   ],
   "source": [
    "# Debug: busca por concept e imprime code, concept_name e name_type, agrupando\n",
    "concept = \"Benign vascular neoplasms of infancy or childhood, unspecified\"\n",
    "top_k = 20\n",
    "\n",
    "vector = get_embedding(concept)\n",
    "res = qdrant.query_points(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query=vector,\n",
    "    limit=top_k,\n",
    "    with_payload=True,\n",
    "    query_filter=Filter(\n",
    "        must=[\n",
    "            {\"key\": \"code_type\", \"match\": {\"value\": \"stem\"}}\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Agrupa resultados por c√≥digo\n",
    "grouped = defaultdict(list)\n",
    "for i, r in enumerate(res.points):\n",
    "    code = r.payload.get('code')\n",
    "    name = r.payload.get('concept_name')\n",
    "    name_type = r.payload.get('name_type')\n",
    "    grouped[code].append((name, name_type))\n",
    "\n",
    "print(f\"Resultados agrupados por code para: {concept}\\n\")\n",
    "for code, values in grouped.items():\n",
    "    print(f\"Code: {code} (total: {len(values)})\")\n",
    "    for idx, (name, name_type) in enumerate(values, 1):\n",
    "        print(f\"   {idx:02d}. {name_type}: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Resumo r√°pido de quantos name_types por c√≥digo\n",
    "print(\"\\nResumo por code:\")\n",
    "for code, values in grouped.items():\n",
    "    nt_types = \", \".join(sorted(set(nt for _, nt in values)))\n",
    "    print(f\"- {code}: {len(values)} resultados ({nt_types})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "70f0818f-0e3f-4564-9228-dfb6f0605d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrou XT5R no Qdrant? True\n",
      "\n",
      "Payload retornado para ‚ÄòXT5R‚Äô:\n",
      "{'concept_name': 'Acute', 'code': 'XT5R', 'entity_id_residual': '786106375', 'code_type': 'extension', 'name_type': 'fsn', 'is_leaf': True, 'chapter': [{'code': 'X', 'title': 'Extension Codes'}], 'blocks': [], 'parents': [{'code': '', 'title': 'Acute-Subacute-Chronic Scale Value', 'entity_id_residual': '943864406'}, {'code': '', 'title': 'Course', 'entity_id_residual': '2082570273'}, {'code': '', 'title': 'Course of the Condition', 'entity_id_residual': '1770621312'}, {'code': '', 'title': 'Temporality', 'entity_id_residual': '614922797'}, {'code': 'X', 'title': 'Extension Codes', 'entity_id_residual': '979408586'}, {'code': '', 'title': 'ICD Category', 'entity_id_residual': '455013390'}, {'code': '', 'title': 'ICD Entity', 'entity_id_residual': '448895267'}], 'has_required_postcoordination': False, 'leaf_related_codes': []}\n"
     ]
    }
   ],
   "source": [
    "# Bloco de teste: verificar se ‚ÄúXT5R‚Äù est√° indexado no Qdrant\n",
    "\n",
    "# 1) Importa√ß√µes (caso ainda n√£o estejam no notebook)\n",
    "from qdrant_client.models import Filter\n",
    "\n",
    "# 2) Execu√ß√£o da consulta pontual para ‚ÄúXT5R‚Äù\n",
    "res_xt5r = qdrant.query_points(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    with_payload=True,\n",
    "    limit=1,\n",
    "    query_filter=Filter(\n",
    "        must=[\n",
    "            {\"key\": \"code\", \"match\": {\"value\": \"XT5R\"}}\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 3) Exibir resultado\n",
    "print(\"Encontrou XT5R no Qdrant?\", bool(res_xt5r.points))\n",
    "if res_xt5r.points:\n",
    "    print(\"\\nPayload retornado para ‚ÄòXT5R‚Äô:\")\n",
    "    print(res_xt5r.points[0].payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a77be-6d2e-48bf-a4d7-faff78129987",
   "metadata": {},
   "source": [
    "#### 4. Testing (QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9fbab004-618b-4fb1-b6ad-5477e62b6446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Total de linhas: 168414\n",
      "üìÇ Divis√£o por categoria:\n",
      "  source:icdapi>synonym>same-as+stem: 82673\n",
      "  source:icdapi>same-as+stem: 16019\n",
      "  source:icdapi>extension: 14947\n",
      "  source:ciel>paraphrase>narrower-than+stem: 10206\n",
      "  source:ciel>narrower-than+stem: 9830\n",
      "  source:ciel>paraphrase>same-as+stem: 5329\n",
      "  source:ciel>same-as+stem: 5165\n",
      "  source:icdapi>synonym>extension: 4987\n",
      "  source:ciel>paraphrase>narrower-than+stem+extension: 4156\n",
      "  source:ciel>narrower-than+stem+extension: 3420\n",
      "  source:ciel>paraphrase>same-as+stem+extension: 3390\n",
      "  source:ciel>same-as+stem+extension: 2866\n",
      "  source:ciel>paraphrase>broader-than+stem: 1355\n",
      "  source:ciel>broader-than+stem: 1280\n",
      "  source:ciel>paraphrase>broader-than+stem+extension: 525\n",
      "  source:ciel>broader-than+stem+extension: 438\n",
      "  source:ciel>paraphrase>same-as+stem+extensions: 376\n",
      "  source:ciel>paraphrase>same-as+cluster: 353\n",
      "  source:ciel>same-as+cluster: 316\n",
      "  source:ciel>same-as+stem+extensions: 282\n",
      "  source:ciel>paraphrase>same-as+cluster+extension: 161\n",
      "  source:ciel>same-as+cluster+extension: 136\n",
      "  source:ciel>paraphrase>narrower-than+stem+extensions: 94\n",
      "  source:ciel>narrower-than+stem+extensions: 77\n",
      "  source:ciel>paraphrase>broader-than+stem+extensions: 20\n",
      "  source:ciel>broader-than+stem+extensions: 13\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Caminho para o arquivo\n",
    "FILE_PATH = Path(\"icd11_alpaca_v2_augmented_paraphrases.jsonl\")\n",
    "\n",
    "# Inicializa contadores\n",
    "total = 0\n",
    "categories = Counter()\n",
    "\n",
    "# L√™ o arquivo linha a linha\n",
    "with FILE_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            category = item.get(\"category\", \"undefined\")\n",
    "            categories[category] += 1\n",
    "            total += 1\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Erro na linha {total + 1}: {e}\")\n",
    "\n",
    "# Exibe resultado\n",
    "print(f\"\\nüìä Total de linhas: {total}\")\n",
    "print(\"üìÇ Divis√£o por categoria:\")\n",
    "for cat, count in categories.most_common():\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5fd6adc5-0237-4a65-98ce-5ec90cdf5625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas lidas: 168414\n",
      "  ‚Üí Train:      134731\n",
      "  ‚Üí Validation: 16841\n",
      "  ‚Üí Test:       16842\n",
      "Arquivos gerados em: .\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# ‚îÄ‚îÄ 1. Par√¢metros e caminhos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "INPUT_FILE  = Path(\"icd11_alpaca_v2_augmented_paraphrases.jsonl\")\n",
    "OUTPUT_DIR  = Path(\".\")  # pode ajustar para outra pasta\n",
    "SEED        = 1234       # para reprodutibilidade\n",
    "TRAIN_PCT   = 0.80\n",
    "VAL_PCT     = 0.10\n",
    "TEST_PCT    = 0.10\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "# ‚îÄ‚îÄ 2. Carrega e embaralha todas as linhas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "all_lines = []\n",
    "with INPUT_FILE.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            # verifica se √© um JSON v√°lido\n",
    "            json.loads(line)\n",
    "            all_lines.append(line)\n",
    "        except json.JSONDecodeError:\n",
    "            # ignora eventuais linhas malformadas\n",
    "            continue\n",
    "\n",
    "random.shuffle(all_lines)\n",
    "\n",
    "# ‚îÄ‚îÄ 3. Calcula pontos de corte para as divis√µes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "N = len(all_lines)\n",
    "n_train = int(N * TRAIN_PCT)\n",
    "n_val   = int(N * VAL_PCT)\n",
    "# n_test ser√° o restante\n",
    "n_test  = N - n_train - n_val\n",
    "\n",
    "train_lines      = all_lines[:n_train]\n",
    "validation_lines = all_lines[n_train : n_train + n_val]\n",
    "test_lines       = all_lines[n_train + n_val :]\n",
    "\n",
    "print(f\"Total de linhas lidas: {N}\")\n",
    "print(f\"  ‚Üí Train:      {len(train_lines)}\")\n",
    "print(f\"  ‚Üí Validation: {len(validation_lines)}\")\n",
    "print(f\"  ‚Üí Test:       {len(test_lines)}\")\n",
    "\n",
    "# ‚îÄ‚îÄ 4. Salva em tr√™s arquivos JSONL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def dump_jsonl(lines, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as out_f:\n",
    "        for ln in lines:\n",
    "            out_f.write(ln + \"\\n\")\n",
    "\n",
    "dump_jsonl(train_lines,      OUTPUT_DIR / \"train.jsonl\")\n",
    "dump_jsonl(validation_lines, OUTPUT_DIR / \"validation.jsonl\")\n",
    "dump_jsonl(test_lines,       OUTPUT_DIR / \"test.jsonl\")\n",
    "\n",
    "print(\"Arquivos gerados em:\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
